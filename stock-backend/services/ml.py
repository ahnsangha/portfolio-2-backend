# ML ë¶„ì„ ëª¨ë“ˆ (PCA/KMeans/HRP/IVP/MinVar/GRU/RF) 
# ì…ë ¥: returns(DataFrame: ë‚ ì§œ, í‹°ì»¤), prices(DataFrame ì„ íƒ) 
# ëª©í‘œ: ë‹¤ìŒ horizon ì˜ì—…ì¼ í‰ê·  ìˆ˜ìµë¥ ì„ ë‹¨ë©´ ë­í‚¹ìœ¼ë¡œ ì˜ˆì¸¡ ë° í‰ê°€(IC/Hit)
# ì•ˆì •ì„±: ì‹œë“œ ê³ ì •, gradient clipping, LR ìŠ¤ì¼€ì¤„ëŸ¬, ì˜ˆì™¸ ë¡œê¹… 
# ìë™í™”: ìœ ë‹ˆë²„ìŠ¤ í¬ê¸° ê¸°ë°˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ ì„¤ì •Â·ë©”íƒ€ íŠœë‹

from __future__ import annotations # íƒ€ì… íŒíŠ¸ í˜¸í™˜ì„±
from dataclasses import dataclass # ë°ì´í„° í´ë˜ìŠ¤ ì •ì˜ìš©
from typing import Dict, Any, Optional, List, Tuple # íƒ€ì… íŒíŠ¸ìš©
import logging # ë¡œê¹… ëª¨ë“ˆ
import numpy as np # ìˆ˜ì¹˜ ê³„ì‚°
import pandas as pd # ë°ì´í„°í”„ë ˆì„

from sklearn.decomposition import PCA # ì£¼ì„±ë¶„ ë¶„ì„
from sklearn.cluster import KMeans # K-í‰ê·  í´ëŸ¬ìŠ¤í„°ë§
from sklearn.preprocessing import StandardScaler # í‘œì¤€í™”
from sklearn.ensemble import RandomForestRegressor # ëœë¤í¬ë ˆìŠ¤íŠ¸ íšŒê·€
from sklearn.metrics import r2_score # Rì œê³± ì ìˆ˜
from sklearn.covariance import LedoitWolf # Ledoit-Wolf ê³µë¶„ì‚° ì¶”ì •

from scipy.cluster.hierarchy import linkage, leaves_list # ê³„ì¸µ í´ëŸ¬ìŠ¤í„°ë§
from scipy.spatial.distance import squareform # ê±°ë¦¬í–‰ë ¬ ë³€í™˜

logger = logging.getLogger(__name__) # ë¡œê±° ì¸ìŠ¤í„´ìŠ¤

# ğŸ’¡ [ìµœì í™” 1] TORCH_AVAILABLE í”Œë˜ê·¸ë¥¼ ì‚¬ìš©í•˜ì—¬ torchë¥¼ ì¡°ê±´ë¶€ë¡œ import í•©ë‹ˆë‹¤.
TORCH_AVAILABLE = False
try:
    import torch
    import torch.nn as nn
    from torch.utils.data import DataLoader, Dataset
    TORCH_AVAILABLE = True
except ImportError:
    logger.debug("Torch not available, GRU model disabled.")
    torch = None
    nn = None
    DataLoader = None
    Dataset = None

# ê²°ê³¼ ì „ë‹¬ìš© ì»¨í…Œì´ë„ˆ 
@dataclass
class PCAResult:
    components: pd.DataFrame # PCA ì£¼ì„±ë¶„ ì¢Œí‘œ
    explained_var: np.ndarray # ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨

    @property
    def explained_variance_ratio_(self): # sklearn PCA í˜¸í™˜ì„± í”„ë¡œí¼í‹°
        return self.explained_var

# ê²°ê³¼ ì „ë‹¬ìš© ì»¨í…Œì´ë„ˆ: í´ëŸ¬ìŠ¤í„° ê²°ê³¼
@dataclass
class ClusterResult:
    labels: pd.Series # ê° ì¢…ëª©ì˜ í´ëŸ¬ìŠ¤í„° ë¼ë²¨
    centers: pd.DataFrame # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì  ì¢Œí‘œ

# ê²°ê³¼ ì „ë‹¬ìš© ì»¨í…Œì´ë„ˆ: HRP ê²°ê³¼
@dataclass
class HRPResult:
    weights: pd.Series # HRP ê°€ì¤‘ì¹˜
    order: List[str] # ê³„ì¸µ ìˆœì„œ

# ê²°ê³¼ ì „ë‹¬ìš© ì»¨í…Œì´ë„ˆ: ì˜ˆì¸¡ ê²°ê³¼
@dataclass
class PredictionResult:
    horizon: int # ì˜ˆì¸¡ ê¸°ê°„
    ic: float # ì •ë³´ ê³„ìˆ˜ (ì˜ˆì¸¡ê³¼ ì‹¤ì œì˜ ìƒê´€ê³„ìˆ˜)
    hit_rate: float # ì ì¤‘ë¥  (ë°©í–¥ ë§ì¶˜ ë¹„ìœ¨)
    r2: Optional[float] # Rì œê³± ì ìˆ˜
    ic_by_date: pd.Series # ë‚ ì§œë³„ IC
    preds_vs_real: pd.DataFrame # ì˜ˆì¸¡ê°’ vs ì‹¤ì œê°’

# ë©”ì¸ ë¶„ì„ê¸° í´ë˜ìŠ¤
class MLAnalyzer:
    def __init__(self, returns: pd.DataFrame, prices: Optional[pd.DataFrame] = None):
        assert isinstance(returns, pd.DataFrame), "returns must be a DataFrame" # DataFrame ì²´í¬
        self.returns = returns.sort_index().copy() # ë‚ ì§œìˆœ ì •ë ¬ í›„ ë³µì‚¬
        self.returns = self.returns.replace([np.inf, -np.inf], np.nan).fillna(0.0) # ë¬´í•œê°’ ì œê±°
        self.prices = None if prices is None else prices.reindex(self.returns.index).copy() # ê°€ê²© ë°ì´í„°

        # í•˜ë‹¨ í‘œì‹œìš© ë©”ì‹œì§€ ë²„í¼ (ì°¨íŠ¸ í•˜ë‹¨ì— ë Œë”ë§í•  í…ìŠ¤íŠ¸)
        self.ui_messages: List[str] = []

    # í•˜ë‹¨ ë©”ì‹œì§€ ë²„í¼ ìœ í‹¸
    def _push_msg(self, text: str) -> None:
        # í•˜ë‹¨ í‘œì‹œìš© ë©”ì‹œì§€ ë²„í¼ì— ì¶”ê°€(ë¹ˆ ì¤„/ì–‘ë ê³µë°± ì •ë¦¬).
        try:
            if not isinstance(text, str): # ë¬¸ìì—´ì´ ì•„ë‹ˆë©´ ë³€í™˜
                text = str(text)
            text = "\n".join(line.rstrip() for line in text.strip().splitlines()) # ê³µë°± ì •ë¦¬
            if text: # ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹ˆë©´ ì¶”ê°€
                self.ui_messages.append(text)
        except Exception as e:
            logger.debug("push_msg failed: %s", e) # ì‹¤íŒ¨ ì‹œ ë””ë²„ê·¸ ë¡œê·¸

    # ìë™ íŠœë‹: ìœ ë‹ˆë²„ìŠ¤/ë°ì´í„° ê¸¸ì´ì— ë§ì¶° í•©ë¦¬ì  ê¸°ë³¸ê°’ ì‚°ì¶œ
    def _auto_tune(
        self,
        horizon: int, # ì˜ˆì¸¡ ê¸°ê°„
        seq_len: Optional[int], # ì‹œí€€ìŠ¤ ê¸¸ì´
        epochs: Optional[int], # ì—í­ ìˆ˜
        hidden_size: Optional[int], # íˆë“  ì‚¬ì´ì¦ˆ
        dropout: Optional[float], # ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
        hrp_max_weight: Optional[float], # HRP ìµœëŒ€ ê°€ì¤‘ì¹˜
        hrp_blend_to_equal: Optional[float], # ê· ë“± ê°€ì¤‘ì¹˜ ë¸”ë Œë”© ë¹„ìœ¨
        long_tau: Optional[float], # ë¡± í¬ì§€ì…˜ ì˜¨ë„ íŒŒë¼ë¯¸í„°
        short_tau: Optional[float], # ìˆ í¬ì§€ì…˜ ì˜¨ë„ íŒŒë¼ë¯¸í„°
    ) -> Dict[str, Any]:
        n_days = len(self.returns) # ì „ì²´ ë°ì´í„° ê¸¸ì´
        n_stk = self.returns.shape[1] # ì¢…ëª© ìˆ˜
        max_seq = max(20, int(min(n_days * 0.6, 160))) # ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´

        if n_stk <= 4: # ì†Œê·œëª¨ ìœ ë‹ˆë²„ìŠ¤
            seq_len = seq_len or min(50, max_seq) # ì‹œí€€ìŠ¤ ê¸¸ì´ 50 ë˜ëŠ” ìµœëŒ€ê°’
            epochs = epochs or 60 # ì—í­ 60
            hidden_size = hidden_size or 32 # íˆë“  ì‚¬ì´ì¦ˆ 32
            dropout = dropout if dropout is not None else 0.20 # ë“œë¡­ì•„ì›ƒ 20%
            hrp_max_weight = hrp_max_weight # ìµœëŒ€ ê°€ì¤‘ì¹˜ ì œí•œ ì—†ìŒ
            hrp_blend_to_equal = 0.0 if hrp_blend_to_equal is None else hrp_blend_to_equal # ê· ë“± ë¸”ë Œë”© ì—†ìŒ
            long_tau = long_tau or 1.0 # ë¡± ì˜¨ë„ 1.0
            short_tau = short_tau or 1.0 # ìˆ ì˜¨ë„ 1.0
        elif n_stk <= 10: # ì¤‘ê°„ ê·œëª¨ ìœ ë‹ˆë²„ìŠ¤
            seq_len = seq_len or min(80, max_seq) # ì‹œí€€ìŠ¤ ê¸¸ì´ 80
            epochs = epochs or 45 # ì—í­ 45
            hidden_size = hidden_size or 48 # íˆë“  ì‚¬ì´ì¦ˆ 48
            dropout = dropout if dropout is not None else 0.15 # ë“œë¡­ì•„ì›ƒ 15%
            hrp_max_weight = 0.35 if hrp_max_weight is None else hrp_max_weight # ìµœëŒ€ ê°€ì¤‘ì¹˜ 35%
            hrp_blend_to_equal = 0.20 if hrp_blend_to_equal is None else hrp_blend_to_equal # ê· ë“± ë¸”ë Œë”© 20%
            long_tau = long_tau or 0.85 # ë¡± ì˜¨ë„ 0.85
            short_tau = short_tau or 0.85 # ìˆ ì˜¨ë„ 0.85
        else: # ëŒ€ê·œëª¨ ìœ ë‹ˆë²„ìŠ¤
            seq_len = seq_len or min(120, max_seq) # ì‹œí€€ìŠ¤ ê¸¸ì´ 120
            epochs = epochs or 30 # ì—í­ 30
            hidden_size = hidden_size or 64 # íˆë“  ì‚¬ì´ì¦ˆ 64
            dropout = dropout if dropout is not None else 0.10 # ë“œë¡­ì•„ì›ƒ 10%
            hrp_max_weight = 0.25 if hrp_max_weight is None else hrp_max_weight # ìµœëŒ€ ê°€ì¤‘ì¹˜ 25%
            hrp_blend_to_equal = 0.30 if hrp_blend_to_equal is None else hrp_blend_to_equal # ê· ë“± ë¸”ë Œë”© 30%
            long_tau = long_tau or 0.70 # ë¡± ì˜¨ë„ 0.70
            short_tau = short_tau or 0.70 # ìˆ ì˜¨ë„ 0.70

        if seq_len >= n_days - horizon - 5: # ì‹œí€€ìŠ¤ê°€ ë„ˆë¬´ ê¸¸ë©´ ì¡°ì •
            seq_len = max(20, int((n_days - horizon - 5) * 0.6))

        return dict( # íŠœë‹ëœ íŒŒë¼ë¯¸í„° ë°˜í™˜
            seq_len=seq_len, epochs=epochs, hidden_size=hidden_size, dropout=dropout,
            hrp_max_weight=hrp_max_weight, hrp_blend_to_equal=hrp_blend_to_equal,
            long_tau=long_tau, short_tau=short_tau
        )

    # íŠ¹ì§• ìƒì„±: ëª¨ë©˜í…€Â·ë³€ë™ì„±, ì™œë„, ì²¨ë„, ë² íƒ€
    def build_features(self, windows=(5, 20, 60)) -> pd.DataFrame:
        parts = [] # íŠ¹ì§•ë“¤ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
        for w in windows: # ê° ìœˆë„ìš°ì— ëŒ€í•´
            m = self.returns.rolling(w).mean() # ëª¨ë©˜í…€ (ì´ë™í‰ê· )
            m.columns = pd.MultiIndex.from_product([m.columns, [f"mom{w}"]]) # ì»¬ëŸ¼ëª… ì„¤ì •

            v = self.returns.rolling(w).std() # ë³€ë™ì„± (ì´ë™í‘œì¤€í¸ì°¨)
            v.columns = pd.MultiIndex.from_product([v.columns, [f"vol{w}"]]) # ì»¬ëŸ¼ëª… ì„¤ì •

            sk = self.returns.rolling(w).skew() # ì™œë„ (ë¹„ëŒ€ì¹­ë„)
            sk.columns = pd.MultiIndex.from_product([sk.columns, [f"skew{w}"]]) # ì»¬ëŸ¼ëª… ì„¤ì •

            ku = self.returns.rolling(w).kurt() # ì²¨ë„ (ë¾°ì¡±í•¨)
            ku.columns = pd.MultiIndex.from_product([ku.columns, [f"kurt{w}"]]) # ì»¬ëŸ¼ëª… ì„¤ì •

            parts += [m, v, sk, ku] # íŠ¹ì§• ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€

        market_ret = self.returns.mean(axis=1) # ì‹œì¥ ìˆ˜ìµë¥  (í‰ê· )
        betas = {} # ë² íƒ€ ì €ì¥ìš© ë”•ì…”ë„ˆë¦¬
        for c in self.returns.columns: # ê° ì¢…ëª©ì— ëŒ€í•´
            x = market_ret # ì‹œì¥ ìˆ˜ìµë¥ 
            y = self.returns[c] # ê°œë³„ ì¢…ëª© ìˆ˜ìµë¥ 
            cov = (x * y).rolling(60).mean() - x.rolling(60).mean() * y.rolling(60).mean() # ê³µë¶„ì‚°
            var = x.rolling(60).var() # ì‹œì¥ ë¶„ì‚°
            beta = cov / (var.replace(0, np.nan)) # ë² íƒ€ ê³„ì‚°
            betas[c] = beta # ë² íƒ€ ì €ì¥
        beta_df = pd.DataFrame(betas) # ë² íƒ€ DataFrame
        beta_df.columns = pd.MultiIndex.from_product([beta_df.columns, ["beta"]]) # ì»¬ëŸ¼ëª… ì„¤ì •

        X = pd.concat(parts + [beta_df], axis=1) # ëª¨ë“  íŠ¹ì§• ê²°í•©
        X = X.replace([np.inf, -np.inf], np.nan).fillna(0.0) # ë¬´í•œê°’ ì œê±°
        X.columns.names = ["ticker", "feature"] # ì»¬ëŸ¼ ì´ë¦„ ì„¤ì •
        return X

    # PCA 2ì°¨ì› ì¢Œí‘œ
    def pca_2d(self) -> PCAResult:
        feats = self.build_features() # íŠ¹ì§• ìƒì„±
        last_row = feats.tail(1).iloc[0] # ë§ˆì§€ë§‰ ë‚ ì§œ ë°ì´í„°
        last_row.index = last_row.index.set_names(["ticker", "feature"]) # ì¸ë±ìŠ¤ ì´ë¦„ ì„¤ì •
        stock_features = last_row.unstack("feature").fillna(0.0) # ì¢…ëª©ë³„ íŠ¹ì§•ìœ¼ë¡œ ì¬êµ¬ì„±

        scaler = StandardScaler() # í‘œì¤€í™” ê°ì²´
        Z = scaler.fit_transform(stock_features.values).copy() # íŠ¹ì§• í‘œì¤€í™”

        n_comp = int(min(2, Z.shape[0], Z.shape[1])) # ì£¼ì„±ë¶„ ê°œìˆ˜ (ìµœëŒ€ 2ê°œ)
        if n_comp < 1: # ì£¼ì„±ë¶„ì´ ì—†ìœ¼ë©´
            components = pd.DataFrame(np.zeros((Z.shape[0], 2)), # ì˜í–‰ë ¬ ìƒì„±
                                     index=stock_features.index, columns=["PC1", "PC2"])
            explained = np.array([1.0, 0.0]) # ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨
            return PCAResult(components=components, explained_var=explained)

        pca = PCA(n_components=n_comp, random_state=42) # PCA ê°ì²´ ìƒì„±
        coords = pca.fit_transform(Z) # PCA ë³€í™˜
        components = pd.DataFrame(coords, index=stock_features.index, # ì£¼ì„±ë¶„ ì¢Œí‘œ
                                 columns=["PC1"] + (["PC2"] if n_comp == 2 else []))
        if n_comp == 1: # ì£¼ì„±ë¶„ì´ 1ê°œë©´
            components["PC2"] = 0.0 # PC2ë¥¼ 0ìœ¼ë¡œ ì„¤ì •
            explained = np.array([pca.explained_variance_ratio_[0], 0.0]) # ì„¤ëª…ëœ ë¶„ì‚°
        else:
            evr = np.asarray(pca.explained_variance_ratio_) # ì„¤ëª…ëœ ë¶„ì‚° ë¹„ìœ¨
            explained = np.array([float(evr[0]), float(evr[1]) if len(evr) > 1 else 0.0])
        return PCAResult(components=components[["PC1", "PC2"]], explained_var=explained)

    # KMeans
    def kmeans_clusters(self, pca_result: Optional[PCAResult] = None, k_clusters: int = 4) -> ClusterResult:
        if pca_result is None: # PCA ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì‹¤í–‰
            pca_result = self.pca_2d()
        coords = pca_result.components.values # PCA ì¢Œí‘œ
        n_samples = coords.shape[0] # ìƒ˜í”Œ ìˆ˜
        k = max(1, min(k_clusters, n_samples)) # í´ëŸ¬ìŠ¤í„° ìˆ˜ ì¡°ì •
        try:
            km = KMeans(n_clusters=k, random_state=42, n_init="auto") # KMeans ê°ì²´
        except TypeError: # n_init="auto" ì§€ì› ì•ˆí•˜ë©´
            km = KMeans(n_clusters=k, random_state=42, n_init=10) # ê¸°ë³¸ê°’ ì‚¬ìš©
        labels = km.fit_predict(coords) # í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰
        centers = pd.DataFrame(km.cluster_centers_, columns=["PC1", "PC2"][: km.cluster_centers_.shape[1]]) # ì¤‘ì‹¬ì 
        return ClusterResult(labels=pd.Series(labels, index=pca_result.components.index), # ë¼ë²¨
                            centers=centers) # ì¤‘ì‹¬ì 

    # ìº¡/í”Œë¡œì–´ ë° ì •ê·œí™”
    def _apply_caps(self, w: pd.Series, max_weight: Optional[float], min_weight: float) -> pd.Series:
        w = w.astype(float).copy().clip(lower=min_weight) # ìµœì†Œ ê°€ì¤‘ì¹˜ ì ìš©
        if max_weight is not None: # ìµœëŒ€ ê°€ì¤‘ì¹˜ê°€ ìˆìœ¼ë©´
            w = pd.Series(np.minimum(w.to_numpy(copy=True), float(max_weight)), index=w.index) # ìµœëŒ€ ê°€ì¤‘ì¹˜ ì ìš©
        s = float(w.sum()) # ê°€ì¤‘ì¹˜ í•©ê³„
        if not np.isfinite(s) or s <= 0: # í•©ê³„ê°€ ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´
            return pd.Series(1.0 / len(w), index=w.index) # ê· ë“± ê°€ì¤‘ì¹˜ ë°˜í™˜
        return w / s # ì •ê·œí™”

    # IVP
    def _weights_ivp(self, cov: pd.DataFrame) -> pd.Series:
        d = np.asarray(cov.values, dtype=float).diagonal().copy() # ëŒ€ê°ì„  (ë¶„ì‚°)
        d[d <= 0] = np.nan # 0 ì´í•˜ëŠ” NaN
        ivp = 1.0 / d # ì—­ë¶„ì‚° ê°€ì¤‘ì¹˜
        ivp = np.nan_to_num(ivp, nan=0.0) # NaNì„ 0ìœ¼ë¡œ
        if ivp.sum() == 0: # í•©ê³„ê°€ 0ì´ë©´
            ivp = np.ones_like(ivp) # ê· ë“± ê°€ì¤‘ì¹˜
        ivp = ivp / ivp.sum() # ì •ê·œí™”
        return pd.Series(ivp, index=cov.index)

    # MinVar
    def _weights_minvar(self, cov: pd.DataFrame) -> pd.Series:
        try:
            inv = np.linalg.pinv(cov.values) # ê³µë¶„ì‚° ì—­í–‰ë ¬ (ì˜ì‚¬ì—­í–‰ë ¬)
            ones = np.ones(len(cov)) # 1ë²¡í„°
            w = inv @ ones # ìµœì†Œë¶„ì‚° ê°€ì¤‘ì¹˜
            w = w / (ones @ inv @ ones) # ì •ê·œí™”
            w = pd.Series(w, index=cov.index) # Seriesë¡œ ë³€í™˜
        except Exception as e:
            logger.exception("MinVar weight failed; falling back to equal-weight. Reason: %s", e) # ì‹¤íŒ¨ ë¡œê·¸
            w = pd.Series(1.0 / len(cov), index=cov.index) # ê· ë“± ê°€ì¤‘ì¹˜ë¡œ í´ë°±
        return w

    # HRP/IVP/MinVar í†µí•©
    def hrp_weights(self,
                    scheme: str = "hrp", # ê°€ì¤‘ì¹˜ ë°©ì‹
                    max_weight: Optional[float] = 0.25, # ìµœëŒ€ ê°€ì¤‘ì¹˜
                    min_weight: float = 0.0, # ìµœì†Œ ê°€ì¤‘ì¹˜
                    blend_to_equal: float = 0.30, # ê· ë“± ê°€ì¤‘ì¹˜ ë¸”ë Œë”© ë¹„ìœ¨
                    resamples: int = 0, # ë¦¬ìƒ˜í”Œë§ íšŸìˆ˜
                    seed: int = 42) -> HRPResult: # ì‹œë“œ
        rng = np.random.default_rng(seed) # ëœë¤ ìƒì„±ê¸°
        R = self.returns.fillna(0.0) # NaNì„ 0ìœ¼ë¡œ ì±„ì›€

        def _one(df: pd.DataFrame) -> pd.Series: # ë‹¨ì¼ ê°€ì¤‘ì¹˜ ê³„ì‚° í•¨ìˆ˜
            cov = pd.DataFrame(LedoitWolf().fit(df).covariance_, index=df.columns, columns=df.columns) # Ledoit-Wolf ê³µë¶„ì‚°
            if scheme.lower() == "ivp": # IVP ë°©ì‹
                w = self._weights_ivp(cov)
                order = list(w.index)
            elif scheme.lower() == "minvar": # MinVar ë°©ì‹
                w = self._weights_minvar(cov)
                order = list(w.index)
            else: # HRP ë°©ì‹
                corr = df.corr().fillna(0.0) # ìƒê´€ê³„ìˆ˜ í–‰ë ¬
                dist = np.sqrt(0.5 * (1 - corr)).clip(lower=0) # ê±°ë¦¬ í–‰ë ¬
                Z = linkage(squareform(dist, checks=False), method="ward") # ê³„ì¸µ í´ëŸ¬ìŠ¤í„°ë§
                order_idx = list(leaves_list(Z)) # ë¦¬í”„ ìˆœì„œ
                order = [df.columns[i] for i in order_idx] # ì»¬ëŸ¼ ìˆœì„œ
                w = self._hrp_allocation(cov, order_idx) # HRP ê°€ì¤‘ì¹˜

            if blend_to_equal and blend_to_equal > 0: # ê· ë“± ê°€ì¤‘ì¹˜ ë¸”ë Œë”©
                w = (1 - blend_to_equal) * w + (blend_to_equal) * (pd.Series(1.0, index=w.index) / len(w))
            w = self._apply_caps(w, max_weight, min_weight) # ìº¡/í”Œë¡œì–´ ì ìš©
            return w.sort_values(ascending=False) # ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬

        if resamples and resamples > 0: # ë¦¬ìƒ˜í”Œë§ ì‚¬ìš©
            ws = [] # ê°€ì¤‘ì¹˜ ë¦¬ìŠ¤íŠ¸
            for _ in range(resamples): # ë¦¬ìƒ˜í”Œë§ íšŸìˆ˜ë§Œí¼
                idx = rng.choice(len(R), size=len(R), replace=True) # ë¶€íŠ¸ìŠ¤íŠ¸ë© ì¸ë±ìŠ¤
                ws.append(_one(R.iloc[idx])) # ê°€ì¤‘ì¹˜ ê³„ì‚°
            w = pd.concat(ws, axis=1).mean(axis=1) # í‰ê·  ê°€ì¤‘ì¹˜
            w = w / w.sum() # ì •ê·œí™”
            order = list(w.index) # ìˆœì„œ
            return HRPResult(weights=w.sort_values(ascending=False), order=order)

        w = _one(R) # ë‹¨ì¼ ê°€ì¤‘ì¹˜ ê³„ì‚°
        order = list(w.index) # ìˆœì„œ
        return HRPResult(weights=w.sort_values(ascending=False), order=order)

    # HRP ë‚´ë¶€: ì¬ê·€ ë¶„í•  
    def _hrp_allocation(self, cov: pd.DataFrame, order_idx: List[int]) -> pd.Series:
        items = [self.returns.columns[i] for i in order_idx] # ìˆœì„œëŒ€ë¡œ ì¢…ëª©ëª…
        w = pd.Series(1.0, index=items, dtype=float) # ì´ˆê¸° ê°€ì¤‘ì¹˜ 1
        clusters = [items] # í´ëŸ¬ìŠ¤í„° ë¦¬ìŠ¤íŠ¸
        while clusters: # í´ëŸ¬ìŠ¤í„°ê°€ ìˆëŠ” ë™ì•ˆ
            new_clusters = [] # ìƒˆ í´ëŸ¬ìŠ¤í„° ë¦¬ìŠ¤íŠ¸
            for cl in clusters: # ê° í´ëŸ¬ìŠ¤í„°ì— ëŒ€í•´
                if len(cl) <= 2: # 2ê°œ ì´í•˜ë©´ ìŠ¤í‚µ
                    continue
                mid = len(cl) // 2 # ì¤‘ê°„ì 
                left, right = cl[:mid], cl[mid:] # ì¢Œìš° ë¶„í• 
                w_left = self._cluster_risk(cov, left) # ì¢Œì¸¡ ë¦¬ìŠ¤í¬
                w_right = self._cluster_risk(cov, right) # ìš°ì¸¡ ë¦¬ìŠ¤í¬
                alpha = 1 - w_left / (w_left + w_right + 1e-12) # ë°°ë¶„ ë¹„ìœ¨
                w.loc[left]  = w.loc[left].to_numpy(copy=True)  * alpha # ì¢Œì¸¡ ê°€ì¤‘ì¹˜ ì¡°ì •
                w.loc[right] = w.loc[right].to_numpy(copy=True) * (1 - alpha) # ìš°ì¸¡ ê°€ì¤‘ì¹˜ ì¡°ì •
                new_clusters.extend([left, right]) # ìƒˆ í´ëŸ¬ìŠ¤í„°ì— ì¶”ê°€
            clusters = new_clusters # í´ëŸ¬ìŠ¤í„° ì—…ë°ì´íŠ¸
        return w / w.sum() # ì •ê·œí™”

    # HRP ë‚´ë¶€: í´ëŸ¬ìŠ¤í„° ìœ„í—˜
    def _cluster_risk(self, cov: pd.DataFrame, items: List[str]) -> float:
        sub = cov.loc[items, items] # ì„œë¸Œ ê³µë¶„ì‚° í–‰ë ¬
        ivp = self._weights_ivp(sub) # IVP ê°€ì¤‘ì¹˜
        risk = float(ivp @ sub.values @ ivp.T) # ìœ„í—˜ ê³„ì‚°
        return risk

    # ì§„ë‹¨/ê²€ì¦ ë³´ì¡°
    @staticmethod
    def _winsorize(df: pd.DataFrame, k: float = 3.0) -> pd.DataFrame:
        return df.clip(lower=-float(k), upper=float(k)) # ê·¹ê°’ ì œê±° (k í‘œì¤€í¸ì°¨)

    def ic_decay(self,
                 horizons: Tuple[int, ...] = (1, 3, 5, 10), # ì˜ˆì¸¡ ê¸°ê°„ë“¤
                 use_gru: bool = True, # GRU ì‚¬ìš© ì—¬ë¶€
                 **kwargs) -> pd.DataFrame:
        rows = [] # ê²°ê³¼ í–‰ë“¤
        for h in horizons: # ê° ê¸°ê°„ì— ëŒ€í•´
            pr = self.predict_next(horizon=h, use_gru=use_gru, **kwargs) # ì˜ˆì¸¡ ì‹¤í–‰
            rows.append({"horizon": int(h), "ic": float(pr.ic), "hit": float(pr.hit_rate), "r2": pr.r2}) # ê²°ê³¼ ì €ì¥
        out = pd.DataFrame(rows).set_index("horizon").sort_index() # DataFrame ìƒì„±
        return out

    def permutation_test(self,
                         horizon: int = 5, # ì˜ˆì¸¡ ê¸°ê°„
                         use_gru: bool = True, # GRU ì‚¬ìš© ì—¬ë¶€
                         n_perm: int = 200, # ìˆœì—´ íšŸìˆ˜
                         seed: int = 42, # ì‹œë“œ
                         **kwargs) -> Dict[str, Any]:
        rng = np.random.default_rng(seed) # ëœë¤ ìƒì„±ê¸°
        base = self.predict_next(horizon=horizon, use_gru=use_gru, **kwargs) # ê¸°ë³¸ ì˜ˆì¸¡
        df = base.preds_vs_real.copy() # ì˜ˆì¸¡ vs ì‹¤ì œ ë°ì´í„°
        if df.empty or not isinstance(df.index, pd.MultiIndex): # ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ë©€í‹°ì¸ë±ìŠ¤ê°€ ì•„ë‹ˆë©´
            return {"base_ic": float(base.ic), "null_ic_mean": np.nan, "null_ic_std": np.nan}

        null_ics: List[float] = [] # ë„ IC ë¦¬ìŠ¤íŠ¸
        dates = df.index.get_level_values(0).unique() # ìœ ë‹ˆí¬í•œ ë‚ ì§œë“¤
        for _ in range(int(n_perm)): # ìˆœì—´ íšŸìˆ˜ë§Œí¼
            ics = [] # IC ë¦¬ìŠ¤íŠ¸
            for d in dates: # ê° ë‚ ì§œì— ëŒ€í•´
                g = df.xs(d, level=0) # í•´ë‹¹ ë‚ ì§œ ë°ì´í„°
                y = g["real"].to_numpy(copy=True) # ì‹¤ì œê°’
                rng.shuffle(y) # ì‹¤ì œê°’ ì…”í”Œ
                ic = pd.Series(g["pred"].values).corr(pd.Series(y), method="spearman") # ìˆœìœ„ ìƒê´€ê³„ìˆ˜
                ics.append(ic) # IC ì¶”ê°€
            null_ics.append(float(np.nanmean(ics))) # í‰ê·  IC ì¶”ê°€
        return {
            "base_ic": float(base.ic), # ê¸°ë³¸ IC
            "null_ic_mean": float(np.nanmean(null_ics)), # ë„ IC í‰ê· 
            "null_ic_std": float(np.nanstd(null_ics)), # ë„ IC í‘œì¤€í¸ì°¨
            "n_perm": int(n_perm) # ìˆœì—´ íšŸìˆ˜
        }

    # ê³µí†µ ìœ í‹¸ë¦¬í‹°
    @staticmethod
    def _set_seeds(seed: int = 42, deterministic: bool = True) -> None:
        try:
            np.random.seed(seed) # ë„˜íŒŒì´ ì‹œë“œ ì„¤ì •
        except Exception as e:
            logger.debug("Numpy seed set failed: %s", e)
        if TORCH_AVAILABLE: # í† ì¹˜ê°€ ì‚¬ìš© ê°€ëŠ¥í•˜ë©´
            try:
                torch.manual_seed(seed) # í† ì¹˜ ì‹œë“œ ì„¤ì •
                if torch.cuda.is_available(): # CUDA ì‚¬ìš© ê°€ëŠ¥í•˜ë©´
                    torch.cuda.manual_seed_all(seed) # CUDA ì‹œë“œ ì„¤ì •
                if deterministic: # ê²°ì •ë¡ ì  ëª¨ë“œ
                    try:
                        torch.backends.cudnn.deterministic = True # cuDNN ê²°ì •ë¡ ì 
                        torch.backends.cudnn.benchmark = False # cuDNN ë²¤ì¹˜ë§ˆí¬ ë¹„í™œì„±í™”
                    except Exception as e:
                        logger.debug("cuDNN deterministic flags failed: %s", e)
            except Exception as e:
                logger.debug("Torch seed set failed: %s", e)

    # ì˜ˆì¸¡ ë©”ì¸
    def predict_next(self,
                     horizon: int = 5, # ì˜ˆì¸¡ ê¸°ê°„
                     seq_len: int = 120, # ì‹œí€€ìŠ¤ ê¸¸ì´
                     use_gru: bool = True, # GRU ì‚¬ìš© ì—¬ë¶€
                     epochs: int = 30, # í•™ìŠµ ì—í­ ìˆ˜
                     hidden_size: int = 64, # íˆë“  ë ˆì´ì–´ í¬ê¸°
                     num_layers: int = 1, # GRU ë ˆì´ì–´ ìˆ˜
                     dropout: float = 0.1, # ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
                     batch_size: int = 512, # ë°°ì¹˜ í¬ê¸°
                     lr: float = 1e-3, # í•™ìŠµë¥ 
                     winsor_k: float = 3.0, # ìœˆì†Œë¼ì´ì§• ì„ê³„ê°’
                     seed: int = 42, # ëœë¤ ì‹œë“œ
                     deterministic: bool = True, # ê²°ì •ë¡ ì  ëª¨ë“œ
                     grad_clip: float = 1.0, # ê·¸ë¼ë””ì–¸íŠ¸ í´ë¦¬í•‘
                     scheduler_patience: int = 2, # ìŠ¤ì¼€ì¤„ëŸ¬ ì°¸ì„ì„±
                     device: Optional[str] = None) -> PredictionResult: # ë””ë°”ì´ìŠ¤
        self._set_seeds(seed=seed, deterministic=deterministic) # ì‹œë“œ ì„¤ì •

        if use_gru and not TORCH_AVAILABLE: # GRU ì‚¬ìš©í•˜ì§€ë§Œ í† ì¹˜ ì—†ìœ¼ë©´
            use_gru = False # GRU ë¹„í™œì„±í™”
            self._push_msg("PyTorch not available, falling back to RandomForest") # ë©”ì‹œì§€ ì¶”ê°€
        if len(self.returns) < (seq_len + horizon + 20): # ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´
            self._push_msg(f"Insufficient data for seq_len={seq_len}, using RandomForest") # ë©”ì‹œì§€ ì¶”ê°€
            return self._predict_with_rf(horizon=horizon) # RFë¡œ í´ë°±
        if not use_gru: # GRU ì‚¬ìš© ì•ˆí•˜ë©´
            self._push_msg("Using RandomForest for prediction") # ë©”ì‹œì§€ ì¶”ê°€
            return self._predict_with_rf(horizon=horizon) # RF ì‚¬ìš©

        df_ret_raw = self.returns.copy() # ì›ë³¸ ìˆ˜ìµë¥  ë³µì‚¬
        tickers = df_ret_raw.columns.tolist() # í‹°ì»¤ ë¦¬ìŠ¤íŠ¸
        dates = df_ret_raw.index # ë‚ ì§œ ì¸ë±ìŠ¤

        mu = df_ret_raw.mean(axis=1) # ì¼ë³„ í‰ê·  ìˆ˜ìµë¥ 
        sigma = df_ret_raw.std(axis=1).replace(0, np.nan) # ì¼ë³„ í‘œì¤€í¸ì°¨
        df_ret = (df_ret_raw.sub(mu, axis=0)).div(sigma, axis=0).fillna(0.0) # í‘œì¤€í™”

        fut = np.log1p(df_ret_raw).shift(-horizon).rolling(horizon).sum() # ë¯¸ë˜ ëˆ„ì  ë¡œê·¸ ìˆ˜ìµë¥ 
        t_mu = fut.mean(axis=1) # íƒ€ê²Ÿ í‰ê· 
        t_sd = fut.std(axis=1).replace(0, np.nan) # íƒ€ê²Ÿ í‘œì¤€í¸ì°¨
        target = (fut.sub(t_mu, axis=0)).div(t_sd, axis=0).fillna(0.0) # íƒ€ê²Ÿ í‘œì¤€í™”
        if winsor_k is not None and float(winsor_k) > 0: # ìœˆì†Œë¼ì´ì§• ì ìš©
            target = self._winsorize(target, k=float(winsor_k))

        X_list, y_list, d_list, t_list = [], [], [], [] # ë°ì´í„° ë¦¬ìŠ¤íŠ¸ë“¤
        for t in range(seq_len - 1, len(dates) - horizon): # ì‹œí€€ìŠ¤ ë²”ìœ„ë§Œí¼
            window = df_ret.iloc[t - seq_len + 1:t + 1] # ìœˆë„ìš° ë°ì´í„°
            yrow = target.iloc[t] # íƒ€ê²Ÿ í–‰
            x_np = window.values.astype(np.float32) # ì…ë ¥ ë°°ì—´
            y_np = yrow.values.astype(np.float32) # íƒ€ê²Ÿ ë°°ì—´
            for s, tk in enumerate(tickers): # ê° ì¢…ëª©ì— ëŒ€í•´
                X_list.append(x_np[:, s:s+1]) # ì…ë ¥ ì¶”ê°€
                y_list.append(y_np[s]) # íƒ€ê²Ÿ ì¶”ê°€
                d_list.append(t) # ë‚ ì§œ ì¸ë±ìŠ¤ ì¶”ê°€
                t_list.append(tk) # í‹°ì»¤ ì¶”ê°€

        if len(X_list) == 0: # ë°ì´í„°ê°€ ì—†ìœ¼ë©´
            return self._predict_with_rf(horizon=horizon) # RFë¡œ í´ë°±

        X = np.stack(X_list, axis=0).astype(np.float32, copy=True) # ì…ë ¥ ìŠ¤íƒ
        y = np.array(y_list, dtype=np.float32).reshape(-1, 1).copy() # íƒ€ê²Ÿ ë°°ì—´
        d_idx = np.array(d_list, dtype=np.int64) # ë‚ ì§œ ì¸ë±ìŠ¤ ë°°ì—´
        t_arr = np.array(t_list) # í‹°ì»¤ ë°°ì—´

        valid_t = np.arange(seq_len - 1, len(dates) - horizon) # ìœ íš¨í•œ ì‹œê°„ ì¸ë±ìŠ¤
        tr_end = valid_t[int(len(valid_t) * 0.6)] # í›ˆë ¨ ëì 
        va_end = valid_t[int(len(valid_t) * 0.8)] # ê²€ì¦ ëì 

        tr_mask = d_idx <= tr_end # í›ˆë ¨ ë§ˆìŠ¤í¬
        va_mask = (d_idx > tr_end) & (d_idx <= va_end) # ê²€ì¦ ë§ˆìŠ¤í¬
        te_mask = d_idx > va_end # í…ŒìŠ¤íŠ¸ ë§ˆìŠ¤í¬

        if tr_mask.sum() < 100 or te_mask.sum() < 100: # ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´
            return self._predict_with_rf(horizon=horizon) # RFë¡œ í´ë°±

        Xtr, ytr = X[tr_mask], y[tr_mask] # í›ˆë ¨ ë°ì´í„°
        Xva, yva = X[va_mask], y[va_mask] # ê²€ì¦ ë°ì´í„°
        Xte, yte = X[te_mask], y[te_mask] # í…ŒìŠ¤íŠ¸ ë°ì´í„°
        dte, tte = d_idx[te_mask], t_arr[te_mask] # í…ŒìŠ¤íŠ¸ ë‚ ì§œ/í‹°ì»¤

        class SeqDS(Dataset): # ì‹œí€€ìŠ¤ ë°ì´í„°ì…‹ í´ë˜ìŠ¤
            def __init__(self, X, y): # ì´ˆê¸°í™”
                self.X = torch.from_numpy(X) # ì…ë ¥ í…ì„œ
                self.y = torch.from_numpy(y) # íƒ€ê²Ÿ í…ì„œ
            def __len__(self): return len(self.X) # ê¸¸ì´ ë°˜í™˜
            def __getitem__(self, i): return self.X[i], self.y[i] # ì•„ì´í…œ ë°˜í™˜

        train_loader = DataLoader(SeqDS(Xtr, ytr), batch_size=batch_size, shuffle=True, drop_last=False) # í›ˆë ¨ ë¡œë”
        val_loader   = DataLoader(SeqDS(Xva, yva), batch_size=batch_size, shuffle=False, drop_last=False) # ê²€ì¦ ë¡œë”
        test_loader  = DataLoader(SeqDS(Xte, yte), batch_size=batch_size, shuffle=False, drop_last=False) # í…ŒìŠ¤íŠ¸ ë¡œë”

        class GRUReg(nn.Module): # GRU íšŒê·€ ëª¨ë¸
            def __init__(self, input_size=1, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout): # ì´ˆê¸°í™”
                super().__init__() # ë¶€ëª¨ í´ë˜ìŠ¤ ì´ˆê¸°í™”
                self.gru = nn.GRU(input_size, hidden_size, num_layers=num_layers, # GRU ë ˆì´ì–´
                                  batch_first=True, dropout=(dropout if num_layers > 1 else 0.0))
                self.head = nn.Linear(hidden_size, 1) # ì¶œë ¥ ë ˆì´ì–´
            def forward(self, x): # ìˆœì „íŒŒ
                out, _ = self.gru(x) # GRU í†µê³¼
                h = out[:, -1, :] # ë§ˆì§€ë§‰ íˆë“  ìƒíƒœ
                return self.head(h) # ì¶œë ¥ ë ˆì´ì–´ í†µê³¼

        device = device or ("cuda" if (torch and torch.cuda.is_available()) else "cpu") # ë””ë°”ì´ìŠ¤ ì„¤ì •
        model = GRUReg().to(device) # ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ
        opt = torch.optim.Adam(model.parameters(), lr=lr) # Adam ì˜µí‹°ë§ˆì´ì €
        loss_fn = nn.MSELoss() # MSE ì†ì‹¤í•¨ìˆ˜

        scheduler = None # ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™”
        try:
            Scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau # ìŠ¤ì¼€ì¤„ëŸ¬ í´ë˜ìŠ¤
            try:
                scheduler = Scheduler( # ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„± ì‹œë„
                    opt, mode="min", factor=0.5,
                    patience=int(max(1, scheduler_patience)),
                    min_lr=1e-5
                )
            except TypeError: # ì¸ì ì˜¤ë¥˜ ì‹œ
                scheduler = Scheduler( # ê°„ë‹¨í•œ ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„±
                    opt, mode="min", factor=0.5,
                    patience=int(max(1, scheduler_patience))
                )
        except Exception as e:
            logger.debug("LR scheduler init failed: %s", e) # ìŠ¤ì¼€ì¤„ëŸ¬ ì‹¤íŒ¨ ë¡œê·¸
            scheduler = None

        best_val = np.inf # ìµœê³  ê²€ì¦ ì ìˆ˜
        best_state = None # ìµœê³  ëª¨ë¸ ìƒíƒœ
        
        # í•™ìŠµ ì§„í–‰ ë©”ì‹œì§€
        self._push_msg(f"Training {epochs} epochs")
        
        for ep in range(1, epochs + 1): # ì—í­ë§Œí¼ ë°˜ë³µ
            model.train() # í›ˆë ¨ ëª¨ë“œ
            for xb, yb in train_loader: # í›ˆë ¨ ë°°ì¹˜
                xb = xb.to(device); yb = yb.to(device) # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                opt.zero_grad(set_to_none=True) # ê·¸ë¼ë””ì–¸íŠ¸ ì´ˆê¸°í™”
                yp = model(xb) # ì˜ˆì¸¡
                loss = loss_fn(yp, yb) # ì†ì‹¤ ê³„ì‚°
                loss.backward() # ì—­ì „íŒŒ
                if grad_clip and grad_clip > 0: # ê·¸ë¼ë””ì–¸íŠ¸ í´ë¦¬í•‘
                    try:
                        nn.utils.clip_grad_norm_(model.parameters(), max_norm=float(grad_clip)) # í´ë¦¬í•‘ ì ìš©
                    except Exception as e:
                        logger.debug("Grad clip skipped: %s", e)
                opt.step() # ì˜µí‹°ë§ˆì´ì € ìŠ¤í…

            model.eval() # í‰ê°€ ëª¨ë“œ
            va_loss = 0.0 # ê²€ì¦ ì†ì‹¤ ì´ˆê¸°í™”
            with torch.no_grad(): # ê·¸ë¼ë””ì–¸íŠ¸ ë¹„í™œì„±í™”
                for xb, yb in val_loader: # ê²€ì¦ ë°°ì¹˜
                    xb = xb.to(device); yb = yb.to(device) # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                    yp = model(xb) # ì˜ˆì¸¡
                    va_loss += float(loss_fn(yp, yb).item()) * len(xb) # ì†ì‹¤ ëˆ„ì 

            if scheduler is not None: # ìŠ¤ì¼€ì¤„ëŸ¬ê°€ ìˆìœ¼ë©´
                try:
                    scheduler.step(va_loss) # ìŠ¤ì¼€ì¤„ëŸ¬ ìŠ¤í…
                except Exception as e:
                    logger.debug("Scheduler step failed: %s", e)

            if va_loss < best_val: # ê²€ì¦ ì†ì‹¤ì´ ê°œì„ ë˜ë©´
                best_val = va_loss # ìµœê³  ì ìˆ˜ ì—…ë°ì´íŠ¸
                best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()} # ëª¨ë¸ ìƒíƒœ ì €ì¥
                if ep % 10 == 0:  # 10 ì—í­ë§ˆë‹¤ ì§„í–‰ ìƒí™© ì¶œë ¥
                    self._push_msg(f"Epoch {ep}/{epochs}: validation loss improved")

        if best_state is not None: # ìµœê³  ìƒíƒœê°€ ìˆìœ¼ë©´
            model.load_state_dict(best_state) # ëª¨ë¸ ìƒíƒœ ë¡œë“œ

        model.eval() # í‰ê°€ ëª¨ë“œ
        preds = [] # ì˜ˆì¸¡ê°’ ë¦¬ìŠ¤íŠ¸
        with torch.no_grad(): # ê·¸ë¼ë””ì–¸íŠ¸ ë¹„í™œì„±í™”
            for xb, _ in test_loader: # í…ŒìŠ¤íŠ¸ ë°°ì¹˜
                xb = xb.to(device) # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                yp = model(xb).cpu().numpy().reshape(-1) # ì˜ˆì¸¡ í›„ numpyë¡œ
                preds.append(yp) # ì˜ˆì¸¡ê°’ ì¶”ê°€
        preds = np.concatenate(preds, axis=0) # ì˜ˆì¸¡ê°’ ì—°ê²°
        reals = yte.reshape(-1) # ì‹¤ì œê°’ í‰íƒ„í™”

        df_eval = pd.DataFrame({"date_idx": dte, "ticker": tte, "pred": preds, "real": reals}) # í‰ê°€ ë°ì´í„°
        ic_by_date = df_eval.groupby("date_idx").apply( # ë‚ ì§œë³„ IC ê³„ì‚°
            lambda g: g["pred"].corr(g["real"], method="spearman")
        )
        hit_by_date = df_eval.groupby("date_idx").apply( # ë‚ ì§œë³„ ì ì¤‘ë¥  ê³„ì‚°
            lambda g: ((g["pred"] > 0) == (g["real"] > 0)).mean()
        )
        ic_avg = float(np.nanmean(ic_by_date.values)) # í‰ê·  IC
        hit_avg = float(np.nanmean(hit_by_date.values)) # í‰ê·  ì ì¤‘ë¥ 
        try:
            r2 = float(r2_score(reals, preds)) # R2 ì ìˆ˜ ê³„ì‚°
        except Exception as e:
            logger.debug("R2 score failed: %s", e)
            r2 = None

        pred_table = df_eval.copy() # ì˜ˆì¸¡ í…Œì´ë¸” ë³µì‚¬
        pred_table["date"] = pred_table["date_idx"].map(lambda i: dates[int(i)]) # ë‚ ì§œ ë§¤í•‘
        pred_table = pred_table.set_index(["date", "ticker"])[["pred", "real"]].sort_index() # ì¸ë±ìŠ¤ ì„¤ì •

        return PredictionResult( # ì˜ˆì¸¡ ê²°ê³¼ ë°˜í™˜
            horizon=horizon, ic=ic_avg, hit_rate=hit_avg, r2=r2,
            ic_by_date=pd.Series(ic_by_date.values, index=[dates[int(i)] for i in ic_by_date.index]),
            preds_vs_real=pred_table
        )

    def _predict_with_rf(self, horizon: int = 5) -> PredictionResult:
        X = self.build_features() # íŠ¹ì§• ìƒì„±
        y = self.returns.mean(axis=1).shift(-horizon).rolling(horizon).mean() # íƒ€ê²Ÿ ìƒì„± (ë¯¸ë˜ í‰ê·  ìˆ˜ìµë¥ )
        df = pd.concat([X, y.rename("target")], axis=1).dropna() # íŠ¹ì§•ê³¼ íƒ€ê²Ÿ ê²°í•©
        if df.empty: # ë°ì´í„°ê°€ ì—†ìœ¼ë©´
            return PredictionResult( # ë¹ˆ ê²°ê³¼ ë°˜í™˜
                horizon=horizon, ic=float("nan"), hit_rate=float("nan"), r2=None,
                ic_by_date=pd.Series(dtype=float),
                preds_vs_real=pd.DataFrame(columns=["pred", "real"])
            )

        feature_cols = [c for c in df.columns if c != "target"] # íŠ¹ì§• ì»¬ëŸ¼
        split_idx = int(len(df) * 0.6) # ë¶„í•  ì¸ë±ìŠ¤
        Xtr = df.iloc[:split_idx][feature_cols].values # í›ˆë ¨ íŠ¹ì§•
        ytr = df.iloc[:split_idx]["target"].values # í›ˆë ¨ íƒ€ê²Ÿ
        Xte = df.iloc[split_idx:][feature_cols].values # í…ŒìŠ¤íŠ¸ íŠ¹ì§•
        yte = df.iloc[split_idx:]["target"].values # í…ŒìŠ¤íŠ¸ íƒ€ê²Ÿ

        scaler = StandardScaler() # í‘œì¤€í™” ê°ì²´
        Xtr = scaler.fit_transform(Xtr) # í›ˆë ¨ ë°ì´í„° í‘œì¤€í™”
        Xte = scaler.transform(Xte) # í…ŒìŠ¤íŠ¸ ë°ì´í„° í‘œì¤€í™”

        model = RandomForestRegressor(n_estimators=300, random_state=42, n_jobs=-1) # RF ëª¨ë¸
        model.fit(Xtr, ytr) # ëª¨ë¸ í›ˆë ¨
        yp = model.predict(Xte) # ì˜ˆì¸¡

        pred_table = pd.DataFrame({"pred": yp, "real": yte}, index=df.index[split_idx:]) # ì˜ˆì¸¡ í…Œì´ë¸”
        return PredictionResult( # ì˜ˆì¸¡ ê²°ê³¼ ë°˜í™˜
            horizon=horizon, ic=float("nan"), hit_rate=float("nan"),
            r2=float(r2_score(yte, yp)),
            ic_by_date=pd.Series(dtype=float),
            preds_vs_real=pd.DataFrame(columns=["pred", "real"])
        )

    # ì¢…ëª© ì¶”ì²œ ë° í¬íŠ¸í´ë¦¬ì˜¤ êµ¬ì„±
    def _build_picks(self, pred_res: PredictionResult, base_weights: pd.Series,
                     top_n: int, bottom_n: int, # ìƒìœ„/í•˜ìœ„ ê°œìˆ˜
                     long_tau: float = 0.7, short_tau: float = 0.7, # ì˜¨ë„ íŒŒë¼ë¯¸í„°
                     long_only: bool = False) -> Dict[str, Any]: # ë¡±ì˜¨ë¦¬ ì—¬ë¶€
        def _softmax(s: pd.Series, tau: float) -> pd.Series: # ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜
            if len(s) == 0: # ë¹ˆ ì‹œë¦¬ì¦ˆë©´
                return s
            x = (s.to_numpy(dtype=float, copy=True) - np.nanmax(s.values)) / max(tau, 1e-6) # ì •ê·œí™”
            w = np.exp(x) # ì§€ìˆ˜ ë³€í™˜
            denom = w.sum() # ë¶„ëª¨
            if not np.isfinite(denom) or denom <= 0: # ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´
                return pd.Series(0.0, index=s.index) # 0 ë°˜í™˜
            w = w / denom # ì •ê·œí™”
            return pd.Series(w, index=s.index)

        out = {"asof_date": None, "top": [], "bottom": [], "today_weights": {}} # ì¶œë ¥ ì´ˆê¸°í™”
        try:
            df = pred_res.preds_vs_real # ì˜ˆì¸¡ vs ì‹¤ì œ ë°ì´í„°
            if df is None or df.empty or not isinstance(df.index, pd.MultiIndex): # ë°ì´í„° í™•ì¸
                return out

            last_date = df.index.get_level_values(0).max() # ë§ˆì§€ë§‰ ë‚ ì§œ
            snap = df.xs(last_date, level=0).copy() # ë§ˆì§€ë§‰ ë‚ ì§œ ë°ì´í„°
            sig = snap["pred"].astype(float) # ì˜ˆì¸¡ ì‹ í˜¸

            try:
                from services.data import stock_manager # ì£¼ì‹ ë§¤ë‹ˆì € ì„í¬íŠ¸
                name_map = {s["ticker"]: s["name"] for s in stock_manager.get_all_stocks()} # ì´ë¦„ ë§¤í•‘
            except Exception as e:
                logger.debug("stock_manager not available, using ticker as name: %s", e)
                name_map = {t: t for t in sig.index} # í‹°ì»¤ë¥¼ ì´ë¦„ìœ¼ë¡œ ì‚¬ìš©

            base_w = base_weights.reindex(sig.index).fillna(0.0) # ê¸°ë³¸ ê°€ì¤‘ì¹˜

            long_score  = (sig.clip(lower=0.0) * base_w).replace([np.inf, -np.inf], 0.0) # ë¡± ìŠ¤ì½”ì–´
            short_score = ((-sig.clip(upper=0.0)) * base_w).replace([np.inf, -np.inf], 0.0) # ìˆ ìŠ¤ì½”ì–´

            long_w  = _softmax(long_score,  long_tau)  if long_score.sum()  > 0 else pd.Series(0.0, index=sig.index) # ë¡± ê°€ì¤‘ì¹˜
            short_w = _softmax(short_score, short_tau) if short_score.sum() > 0 else pd.Series(0.0, index=sig.index) # ìˆ ê°€ì¤‘ì¹˜

            top_idx = sig.sort_values(ascending=False).head(top_n).index.tolist() # ìƒìœ„ ì¢…ëª©
            bot_idx = sig.sort_values(ascending=True).head(bottom_n).index.tolist() # í•˜ìœ„ ì¢…ëª©

            top = [{ # ìƒìœ„ ì¢…ëª© ì •ë³´
                "rank": i + 1, "ticker": t,
                "name": name_map.get(t, t),
                "score": float(sig[t] * 100.0),
                "weight": float(long_w.get(t, 0.0))
            } for i, t in enumerate(top_idx)]

            bottom = [{ # í•˜ìœ„ ì¢…ëª© ì •ë³´
                "rank": i + 1, "ticker": t,
                "name": name_map.get(t, t),
                "score": float(sig[t] * 100.0),
                "weight": float(short_w.get(t, 0.0))
            } for i, t in enumerate(bot_idx)]

            net_w = long_w if long_only else (long_w - short_w) # ìˆœ ê°€ì¤‘ì¹˜
            out = { # ì¶œë ¥ ë”•ì…”ë„ˆë¦¬
                "asof_date": pd.Timestamp(last_date).strftime("%Y-%m-%d"),
                "top": top,
                "bottom": [] if long_only else bottom,
                "today_weights": {k: float(v) for k, v in net_w.items()}
            }
            return out
        except Exception as e:
            logger.exception("Failed to build picks: %s", e)
            return out

    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ íƒìƒ‰
    def _score_config(self, ic: float, hit: float, weights: Optional[pd.Series]) -> float:
        hit_adj = (float(hit) - 0.5) * 2.0 if np.isfinite(hit) else -1.0 # ì ì¤‘ë¥  ì¡°ì •
        ic_safe = float(ic) if np.isfinite(ic) else -1.0 # IC ì•ˆì „ê°’
        div_pen = 0.0 # ë‹¤ì–‘ì„± í˜ë„í‹°
        if isinstance(weights, pd.Series) and len(weights) > 0: # ê°€ì¤‘ì¹˜ê°€ ìˆìœ¼ë©´
            w = weights.fillna(0.0).clip(lower=0.0) # ìŒìˆ˜ ì œê±°
            w = w / (w.sum() if w.sum() > 0 else 1.0) # ì •ê·œí™”
            hhi = float((w ** 2).sum()) # í—ˆí•€ë‹¬ ì§€ìˆ˜
            n = len(w) # ì¢…ëª© ìˆ˜
            base = 1.0 / n # ê¸°ì¤€ê°’
            div_pen = (hhi - base) / max(1e-9, (1.0 - base)) # ë‹¤ì–‘ì„± í˜ë„í‹°
        return 0.7 * ic_safe + 0.3 * hit_adj - 0.10 * div_pen # ì¢…í•© ì ìˆ˜

    def run_meta_tune(self,
                      use_gru: bool = True, # GRU ì‚¬ìš© ì—¬ë¶€
                      horizon_candidates: Optional[List[int]] = None, # ê¸°ê°„ í›„ë³´ë“¤
                      weight_schemes: Optional[List[str]] = None, # ê°€ì¤‘ì¹˜ ë°©ì‹ë“¤
                      long_only_options: Optional[List[bool]] = None, # ë¡±ì˜¨ë¦¬ ì˜µì…˜ë“¤
                      hrp_max_candidates: Optional[List[Optional[float]]] = None, # HRP ìµœëŒ€ ê°€ì¤‘ì¹˜ í›„ë³´ë“¤
                      hrp_blend_candidates: Optional[List[float]] = None, # HRP ë¸”ë Œë”© í›„ë³´ë“¤
                      tau_candidates: Optional[List[float]] = None, # ì˜¨ë„ í›„ë³´ë“¤
                      max_trials: int = 20, # ìµœëŒ€ ì‹œë„ íšŸìˆ˜
                      random_state: int = 42, # ëœë¤ ìƒíƒœ
                      quick: bool = True) -> Dict[str, Any]: # ë¹ ë¥¸ ëª¨ë“œ
        rng = np.random.default_rng(random_state) # ëœë¤ ìƒì„±ê¸°

        horizon_candidates = horizon_candidates or [3, 5, 10] # ê¸°ê°„ í›„ë³´ë“¤ ê¸°ë³¸ê°’
        weight_schemes = weight_schemes or ["hrp", "ivp", "minvar"] # ê°€ì¤‘ì¹˜ ë°©ì‹ ê¸°ë³¸ê°’
        long_only_options = long_only_options or [False, True] # ë¡±ì˜¨ë¦¬ ì˜µì…˜ ê¸°ë³¸ê°’
        hrp_max_candidates = hrp_max_candidates or [None, 0.35, 0.25] # HRP ìµœëŒ€ ê°€ì¤‘ì¹˜ ê¸°ë³¸ê°’
        hrp_blend_candidates = hrp_blend_candidates or [0.0, 0.2, 0.3, 0.5] # HRP ë¸”ë Œë”© ê¸°ë³¸ê°’
        tau_candidates = tau_candidates or [0.7, 0.85, 1.0] # ì˜¨ë„ í›„ë³´ë“¤ ê¸°ë³¸ê°’

        trials = [] # ì‹œë„ ê²°ê³¼ë“¤
        best = {"score": -1e9} # ìµœê³  ê²°ê³¼

        base_auto = self._auto_tune(horizon=5, seq_len=None, epochs=None, hidden_size=None, # ê¸°ë³¸ ìë™ íŠœë‹
                                    dropout=None, hrp_max_weight=None, hrp_blend_to_equal=None,
                                    long_tau=None, short_tau=None)

        for _ in range(max_trials): # ìµœëŒ€ ì‹œë„ íšŸìˆ˜ë§Œí¼
            h = int(rng.choice(horizon_candidates)) # ëœë¤ ê¸°ê°„
            scheme = str(rng.choice(weight_schemes)) # ëœë¤ ê°€ì¤‘ì¹˜ ë°©ì‹
            long_only = bool(rng.choice(long_only_options)) # ëœë¤ ë¡±ì˜¨ë¦¬
            hrp_max = rng.choice(hrp_max_candidates) # ëœë¤ HRP ìµœëŒ€ê°’
            hrp_blend = float(rng.choice(hrp_blend_candidates)) # ëœë¤ HRP ë¸”ë Œë”©
            tau = float(rng.choice(tau_candidates)) # ëœë¤ ì˜¨ë„

            tuned = self._auto_tune( # íŒŒë¼ë¯¸í„° íŠœë‹
                horizon=h,
                seq_len=None, epochs=None, hidden_size=None, dropout=None,
                hrp_max_weight=(hrp_max if hrp_max is not None else None),
                hrp_blend_to_equal=hrp_blend,
                long_tau=tau, short_tau=tau
            )
            seq_len = tuned["seq_len"] # ì‹œí€€ìŠ¤ ê¸¸ì´
            epochs = 8 if quick else tuned["epochs"] # ì—í­ ìˆ˜ (ë¹ ë¥¸ ëª¨ë“œ)
            hidden = tuned["hidden_size"] # íˆë“  ì‚¬ì´ì¦ˆ
            drop = tuned["dropout"] # ë“œë¡­ì•„ì›ƒ

            try:
                wres = self.hrp_weights( # HRP ê°€ì¤‘ì¹˜ ê³„ì‚°
                    scheme=scheme, max_weight=(None if hrp_max is None else float(hrp_max)),
                    min_weight=0.0, blend_to_equal=hrp_blend, resamples=0
                )
                weights = wres.weights # ê°€ì¤‘ì¹˜
            except Exception as e:
                logger.debug("HRP weights failed during meta tune: %s", e)
                weights = None

            try:
                pres = self.predict_next( # ì˜ˆì¸¡ ì‹¤í–‰
                    horizon=h, use_gru=use_gru,
                    seq_len=seq_len, epochs=epochs, hidden_size=hidden, dropout=drop
                )
                score = self._score_config(pres.ic, pres.hit_rate, weights) # ì ìˆ˜ ê³„ì‚°
                trials.append({ # ì‹œë„ ê²°ê³¼ ì €ì¥
                    "horizon": h, "weight_scheme": scheme, "long_only": long_only,
                    "hrp_max_weight": (None if hrp_max is None else float(hrp_max)),
                    "hrp_blend_to_equal": float(hrp_blend),
                    "tau": tau, "seq_len": seq_len, "epochs": epochs,
                    "hidden_size": hidden, "dropout": drop,
                    "ic": float(pres.ic), "hit_rate": float(pres.hit_rate), "score": float(score)
                })
                if score > best["score"]: # ìµœê³  ì ìˆ˜ë³´ë‹¤ ì¢‹ìœ¼ë©´
                    best = dict(trials[-1]) # ìµœê³  ê²°ê³¼ ì—…ë°ì´íŠ¸
            except Exception as e:
                logger.debug("Meta tune trial failed and was skipped: %s", e)
                continue

        trials_df = pd.DataFrame(trials).sort_values("score", ascending=False).reset_index(drop=True) # ì‹œë„ ê²°ê³¼ DataFrame
        summary = { # ìš”ì•½ ì •ë³´
            "best": best,
            "trials": trials_df.to_dict(orient="records"),
            "n_trials": int(len(trials)),
            "params_default": {
                "base_auto": base_auto,
                "search_space_sizes": {
                    "horizon": len(horizon_candidates),
                    "weight_schemes": len(weight_schemes),
                    "long_only": len(long_only_options),
                    "hrp_max": len(hrp_max_candidates),
                    "hrp_blend": len(hrp_blend_candidates),
                    "tau": len(tau_candidates),
                }
            }
        }
        return summary

# ì „ì²´ íŒŒì´í”„ë¼ì¸
    def run_all(self,
                k_clusters: int = 4, # í´ëŸ¬ìŠ¤í„° ê°œìˆ˜
                horizon: int = 5, # ì˜ˆì¸¡ ê¸°ê°„
                use_gru: bool = True, # GRU ì‚¬ìš© ì—¬ë¶€
                top_n: int = 5, # ìƒìœ„ ì¢…ëª© ê°œìˆ˜
                bottom_n: int = 5, # í•˜ìœ„ ì¢…ëª© ê°œìˆ˜
                weight_scheme: str = "hrp", # ê°€ì¤‘ì¹˜ ë°©ì‹
                long_only: bool = False, # ë¡±ì˜¨ë¦¬ ì—¬ë¶€
                seq_len: Optional[int] = None, # ì‹œí€€ìŠ¤ ê¸¸ì´
                epochs: Optional[int] = None, # ì—í­ ìˆ˜
                hidden_size: Optional[int] = None, # íˆë“  ì‚¬ì´ì¦ˆ
                dropout: Optional[float] = None, # ë“œë¡­ì•„ì›ƒ
                hrp_max_weight: Optional[float] = None, # HRP ìµœëŒ€ ê°€ì¤‘ì¹˜
                hrp_min_weight: float = 0.0, # HRP ìµœì†Œ ê°€ì¤‘ì¹˜
                hrp_blend_to_equal: Optional[float] = None, # HRP ê· ë“± ë¸”ë Œë”©
                hrp_resamples: int = 0, # HRP ë¦¬ìƒ˜í”Œë§
                long_tau: Optional[float] = None, # ë¡± ì˜¨ë„
                short_tau: Optional[float] = None, # ìˆ ì˜¨ë„
                meta_tune: bool = False, # ë©”íƒ€ íŠœë‹ ì—¬ë¶€
                meta_max_trials: int = 20, # ë©”íƒ€ íŠœë‹ ìµœëŒ€ ì‹œë„
                meta_random_state: int = 42, # ë©”íƒ€ íŠœë‹ ëœë¤ ìƒíƒœ
                meta_quick: bool = True, # ë©”íƒ€ íŠœë‹ ë¹ ë¥¸ ëª¨ë“œ
                **gru_kwargs) -> Dict[str, Any]: # GRU ì¶”ê°€ ì¸ìë“¤
        # ì‹œì‘ ë©”ì‹œì§€
        self._push_msg(f"ML Analysis started - {self.returns.shape[1]} stocks, {len(self.returns)} days")
        
        meta_result: Optional[Dict[str, Any]] = None # ë©”íƒ€ íŠœë‹ ê²°ê³¼ ì´ˆê¸°í™”
        if meta_tune: # ë©”íƒ€ íŠœë‹ ì‚¬ìš©í•˜ë©´
            self._push_msg("Running meta-tuning for optimal parameters...") # ë©”ì‹œì§€ ì¶”ê°€
            meta_result = self.run_meta_tune( # ë©”íƒ€ íŠœë‹ ì‹¤í–‰
                use_gru=use_gru, max_trials=meta_max_trials,
                random_state=meta_random_state, quick=meta_quick
            )
            if meta_result and meta_result.get("best"): # ìµœê³  ê²°ê³¼ê°€ ìˆìœ¼ë©´
                b = meta_result["best"] # ìµœê³  ê²°ê³¼
                horizon = int(b["horizon"]) # ê¸°ê°„ ì—…ë°ì´íŠ¸
                weight_scheme = str(b["weight_scheme"]) # ê°€ì¤‘ì¹˜ ë°©ì‹ ì—…ë°ì´íŠ¸
                long_only = bool(b["long_only"]) # ë¡±ì˜¨ë¦¬ ì—…ë°ì´íŠ¸
                hrp_max_weight = b["hrp_max_weight"] # HRP ìµœëŒ€ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
                hrp_blend_to_equal = b["hrp_blend_to_equal"] # HRP ë¸”ë Œë”© ì—…ë°ì´íŠ¸
                long_tau = b["tau"]; short_tau = b["tau"] # ì˜¨ë„ ì—…ë°ì´íŠ¸
                seq_len = b["seq_len"]; epochs = b["epochs"] # ì‹œí€€ìŠ¤/ì—í­ ì—…ë°ì´íŠ¸
                hidden_size = b["hidden_size"]; dropout = b["dropout"] # íˆë“ /ë“œë¡­ì•„ì›ƒ ì—…ë°ì´íŠ¸
                self._push_msg(f"Meta-tune best: IC={b.get('ic', 0):.3f}, Hit={b.get('hit_rate', 0):.3f}") # ë©”ì‹œì§€ ì¶”ê°€

        tuned = self._auto_tune( # ìë™ íŠœë‹
            horizon=horizon,
            seq_len=seq_len, epochs=epochs, hidden_size=hidden_size, dropout=dropout,
            hrp_max_weight=hrp_max_weight, hrp_blend_to_equal=hrp_blend_to_equal,
            long_tau=long_tau, short_tau=short_tau
        )
        seq_len     = tuned["seq_len"] if seq_len is None else seq_len # ì‹œí€€ìŠ¤ ê¸¸ì´ ì„¤ì •
        epochs      = tuned["epochs"]  if epochs  is None else epochs # ì—í­ ì„¤ì •
        hidden_size = tuned["hidden_size"] if hidden_size is None else hidden_size # íˆë“  ì‚¬ì´ì¦ˆ ì„¤ì •
        dropout     = tuned["dropout"] if dropout is None else dropout # ë“œë¡­ì•„ì›ƒ ì„¤ì •
        if hrp_blend_to_equal is None: hrp_blend_to_equal = tuned["hrp_blend_to_equal"] # HRP ë¸”ë Œë”© ì„¤ì •
        if long_tau is None: long_tau = tuned["long_tau"] # ë¡± ì˜¨ë„ ì„¤ì •
        if short_tau is None: short_tau = tuned["short_tau"] # ìˆ ì˜¨ë„ ì„¤ì •

        # ê° ë‹¨ê³„ë³„ ì§„í–‰ ìƒí™© ë©”ì‹œì§€
        self._push_msg("Running PCA & clustering analysis...") # PCA í´ëŸ¬ìŠ¤í„°ë§ ë©”ì‹œì§€
        pca_res = self.pca_2d() # PCA ì‹¤í–‰
        clus_res = self.kmeans_clusters(pca_res, k_clusters=min(k_clusters, self.returns.shape[1])) # í´ëŸ¬ìŠ¤í„°ë§ ì‹¤í–‰
        
        self._push_msg(f"Computing {weight_scheme.upper()} portfolio weights...") # ê°€ì¤‘ì¹˜ ê³„ì‚° ë©”ì‹œì§€
        hrp_res  = self.hrp_weights( # HRP ê°€ì¤‘ì¹˜ ê³„ì‚°
            scheme=weight_scheme,
            max_weight=hrp_max_weight, min_weight=hrp_min_weight,
            blend_to_equal=hrp_blend_to_equal, resamples=hrp_resamples
        )
        
        model_name = "GRU" if use_gru else "RandomForest" # ëª¨ë¸ ì´ë¦„
        self._push_msg(f"Training {model_name} model (horizon={horizon}, epochs={epochs})...") # ëª¨ë¸ í›ˆë ¨ ë©”ì‹œì§€
        pred_res = self.predict_next( # ì˜ˆì¸¡ ì‹¤í–‰
            horizon=horizon, use_gru=use_gru,
            seq_len=seq_len, epochs=epochs, hidden_size=hidden_size, dropout=dropout,
            **gru_kwargs
        )
        
        # ê²°ê³¼ ë©”ì‹œì§€ ì¶”ê°€
        ic_val = float(pred_res.ic) if np.isfinite(pred_res.ic) else 0.0 # IC ê°’
        hit_val = float(pred_res.hit_rate) if np.isfinite(pred_res.hit_rate) else 0.0 # ì ì¤‘ë¥  ê°’
        r2_val  = float(pred_res.r2) if (pred_res.r2 is not None and np.isfinite(pred_res.r2)) else 0.0 # R2 ê°’
        self._push_msg(f"Results: IC={ic_val:.3f}, Hit={hit_val:.3f}, RÂ²={r2_val:.3f}") # ê²°ê³¼ ë©”ì‹œì§€

        # íˆ¬ì ë“±ê¸‰ í‰ê°€
        if ic_val > 0.10: # ICê°€ 0.10 ì´ˆê³¼
            self._push_msg("Investment Grade: EXCELLENT - Strong predictive power") # ìš°ìˆ˜ ë“±ê¸‰
        elif ic_val > 0.05: # ICê°€ 0.05 ì´ˆê³¼
            self._push_msg("Investment Grade: GOOD - Acceptable for investment") # ì–‘í˜¸ ë“±ê¸‰
        elif ic_val > 0.02: # ICê°€ 0.02 ì´ˆê³¼
            self._push_msg("Investment Grade: FAIR - Borderline performance") # ë³´í†µ ë“±ê¸‰
        else: # ê·¸ ì™¸
            self._push_msg("Investment Grade: POOR - Not recommended for investment") # ë¶ˆëŸ‰ ë“±ê¸‰

        picks = self._build_picks( # ì¢…ëª© ì„ íƒ
            pred_res, hrp_res.weights,
            top_n=top_n, bottom_n=bottom_n,
            long_tau=long_tau, short_tau=short_tau,
            long_only=long_only
        )

        explained_array = pca_res.explained_var if pca_res.explained_var is not None else pca_res.explained_variance_ratio_ # ì„¤ëª…ëœ ë¶„ì‚°
        explained_list = np.asarray(explained_array).tolist() # ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜

        # NumPy íƒ€ì…ì„ íŒŒì´ì¬ ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
        # PCA ì¢Œí‘œ ë³€í™˜
        pca_coords_records = pca_res.components.reset_index().rename(columns={"index": "ticker"}).to_dict(orient="records")
        cleaned_pca_coords = [
            {k: v if isinstance(v, str) else float(v) for k, v in row.items()}
            for row in pca_coords_records
        ]

        # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì  ë³€í™˜
        centers_records = clus_res.centers.to_dict(orient="records")
        cleaned_centers = [
            {k: float(v) for k, v in row.items()}
            for row in centers_records
        ]
        
        # HRP ê°€ì¤‘ì¹˜ ë³€í™˜
        cleaned_hrp_weights = {
            k: float(v) for k, v in hrp_res.weights.sort_values(ascending=False).to_dict().items()
        }

        out: Dict[str, Any] = {
            "meta": {
                "universe_size": int(self.returns.shape[1]),
                "n_days": int(len(self.returns)),
                "tuned": {
                    "seq_len": seq_len, "epochs": epochs, "hidden_size": hidden_size, "dropout": dropout,
                    "hrp_max_weight": hrp_max_weight, "hrp_blend_to_equal": hrp_blend_to_equal,
                    "long_tau": long_tau, "short_tau": short_tau,
                    "weight_scheme": weight_scheme, "long_only": long_only,
                    "horizon": horizon
                },
                "meta_tune": meta_result or {}
            },
            "pca": {
                "coords": cleaned_pca_coords,
                "explained_var": [float(v) for v in explained_list]
            },
            "clusters": {
                "labels": {k: int(v) for k, v in self._safe_dict(clus_res.labels).items()},
                "centers": cleaned_centers
            },
            "hrp": {
                "weights": cleaned_hrp_weights,
                "order": hrp_res.order
            },
            "prediction": {
                "horizon": int(pred_res.horizon),
                "ic": float(pred_res.ic) if np.isfinite(pred_res.ic) else 0.0,
                "hit_rate": float(pred_res.hit_rate) if np.isfinite(pred_res.hit_rate) else 0.0,
                "r2": float(pred_res.r2) if pred_res.r2 is not None and np.isfinite(pred_res.r2) else None,
                "ic_by_date": {pd.Timestamp(k).strftime("%Y-%m-%d"): float(v) 
                    for k, v in pred_res.ic_by_date.items()},
                # NumPy íƒ€ì…ì„ Python floatìœ¼ë¡œ ë³€í™˜í•˜ì—¬ JSON ì§ë ¬í™” í•´ê²°
                "preds_vs_real": (
                    [
                        {
                            k: (v.isoformat() if isinstance(v, pd.Timestamp) else float(v) if isinstance(v, (np.number, float, int)) else v)
                            for k, v in record.items()
                        }
                        for record in pred_res.preds_vs_real.reset_index().to_dict('records')
                    ]
                    if pred_res.preds_vs_real is not None else None
                )
            },
            "picks": picks,
            "today_weights": picks.get("today_weights", {}),
            "ui_messages": list(getattr(self, "ui_messages", [])),
            "preds_df": pred_res.preds_vs_real.reset_index() if pred_res and hasattr(pred_res, 'preds_vs_real') else None
        }
        return out

    @staticmethod
    def _safe_dict(s: pd.Series) -> Dict[str, Any]: # ì•ˆì „í•˜ê²Œ ë”•ì…”ë„ˆë¦¬ ë³€í™˜
        try:
            return s.to_dict() # ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        except Exception as e:
            logger.debug("_safe_dict fell back to enumerate: %s", e)
            return {str(i): float(v) for i, v in enumerate(s)} # ì¸ë±ìŠ¤ë¡œ ë³€í™˜
    
    # ml_charts.py í˜¸í™˜ìš© alias
    def run_ml_analysis(self, **kwargs) -> Dict[str, Any]:
        # run_allì˜ alias 
        return self.run_all(**kwargs)
    
# ë°ì´í„° í’ˆì§ˆ ìë™ ë¶„ì„ í´ë˜ìŠ¤ 
class AdaptiveMLAnalyzer(MLAnalyzer):
    def auto_detect_data_quality(self) -> Dict[str, Any]:
        # ë°ì´í„° í’ˆì§ˆ ìë™ í‰ê°€ ë° ë©”ì‹œì§€ ìƒì„±
        # ìƒê´€ê´€ê³„ ê³„ì‚°
        corr_matrix = self.returns.corr() # ìƒê´€ê³„ìˆ˜ í–‰ë ¬
        avg_corr = corr_matrix.values[np.triu_indices_from(corr_matrix.values, k=1)].mean() # í‰ê·  ìƒê´€ê³„ìˆ˜

        # PCA ë¶„ì„
        if self.returns.shape[1] >= 3: # ì¢…ëª©ì´ 3ê°œ ì´ìƒì´ë©´
            pca = PCA(n_components=min(3, self.returns.shape[1])) # PCA ê°ì²´
            pca.fit(self.returns.T)  # ì¢…ëª©=ìƒ˜í”Œ, ë‚ ì§œ=í”¼ì²˜
            explained_ratio = float(pca.explained_variance_ratio_[0]) # ì²« ë²ˆì§¸ ì£¼ì„±ë¶„ ì„¤ëª…ë ¥
        else:
            explained_ratio = 0.9  # ì¢…ëª© ìˆ˜ ë¶€ì¡± ê°€ì •
        
        # ë°ì´í„° ì¶©ë¶„ì„± ê³„ì‚°
        n_samples = len(self.returns) # ìƒ˜í”Œ ìˆ˜
        n_features = self.returns.shape[1] # í”¼ì²˜ ìˆ˜
        sample_per_feature = n_samples / max(1, n_features) # í”¼ì²˜ë‹¹ ìƒ˜í”Œ ìˆ˜

        # ë³€ë™ì„± ë¶„ì‚°ë„ ê³„ì‚°
        vol_dispersion = self.returns.std().std() / (self.returns.std().mean() + 1e-8) # ë³€ë™ì„±ì˜ ë³€ë™ì„±

        # ì ìˆ˜í™”
        quality_score = { # í’ˆì§ˆ ì ìˆ˜ ë”•ì…”ë„ˆë¦¬
            "correlation": float(avg_corr),
            "diversity": float(1 - explained_ratio),
            "data_sufficiency": float(min(1.0, sample_per_feature / 100)),
            "volatility_dispersion": float(min(1.0, vol_dispersion * 2)),
        }
        total = ( # ì´ì  ê³„ì‚°
            (1 - abs(quality_score["correlation"])) * 0.30
            + quality_score["diversity"] * 0.30
            + quality_score["data_sufficiency"] * 0.25
            + quality_score["volatility_dispersion"] * 0.15
        )
        quality_score["total"] = float(total) # ì´ì  ì €ì¥

        # ì§„ë‹¨ ë° ê¶Œê³ ì‚¬í•­
        issues, recommendations = [], [] # ë¬¸ì œì ê³¼ ê¶Œê³ ì‚¬í•­ ë¦¬ìŠ¤íŠ¸
        if avg_corr > 0.7: # í‰ê·  ìƒê´€ê³„ìˆ˜ê°€ ë†’ìœ¼ë©´
            issues.append(f"í‰ê·  ìƒê´€ê³„ìˆ˜ {avg_corr:.2f} (ê³¼ë„í•˜ê²Œ ë†’ìŒ)")
            recommendations.append("- ì„¹í„° ë¶„ì‚° ê°•í™” ë˜ëŠ” íŒ©í„° ëª¨ë¸ ê³ ë ¤")
        if explained_ratio > 0.8: # ì²« ë²ˆì§¸ ì£¼ì„±ë¶„ ì„¤ëª…ë ¥ì´ ë†’ìœ¼ë©´
            issues.append(f"PC1 ì„¤ëª…ë ¥ {explained_ratio*100:.1f}% (ë‹¤ì–‘ì„± ë¶€ì¡±)")
            recommendations.append("- ì¢…ëª© ì°¨ë³„í™” ë¶€ì¡±: ê°œë³„ ì˜ˆì¸¡ ë‚œë„ ë†’ìŒ")
        if sample_per_feature < 50: # í”¼ì²˜ë‹¹ ìƒ˜í”Œì´ ë¶€ì¡±í•˜ë©´
            issues.append(f"ì¢…ëª©ë‹¹ í‘œë³¸ {sample_per_feature:.0f}ì¼ (ë¶€ì¡±)")
            recommendations.append(f"- ìµœì†Œ {100 * n_features}ì¼ ì´ìƒ í™•ë³´ ê¶Œì¥")
        if vol_dispersion < 0.3: # ë³€ë™ì„± ë¶„ì‚°ë„ê°€ ë‚®ìœ¼ë©´
            issues.append("ë³€ë™ì„± ë¶„ì‚°ë„ ë‚®ìŒ (ì¢…ëª© ê°„ ë³€ë™ì„± ìœ ì‚¬)")
            recommendations.append("- ë³€ë™ì„± ê¸°ë°˜ ì „ëµ íš¨ê³¼ ì œí•œì ")

        # ì„±ëŠ¥ ê¸°ëŒ€ì¹˜
        if total >= 0.7: # ì´ì ì´ 0.7 ì´ìƒ
            performance_msg = "ì˜ˆìƒ IC: 0.10+ (ìš°ìˆ˜)"
            confidence = "HIGH"
        elif total >= 0.5: # ì´ì ì´ 0.5 ì´ìƒ
            performance_msg = "ì˜ˆìƒ IC: 0.05~0.10 (ì–‘í˜¸)"
            confidence = "MEDIUM"
        elif total >= 0.3: # ì´ì ì´ 0.3 ì´ìƒ
            performance_msg = "ì˜ˆìƒ IC: 0.02~0.05 (ë¯¸í¡)"
            confidence = "LOW"
        else: # ê·¸ ì™¸
            performance_msg = "ì˜ˆìƒ IC: <0.02 (ë¬´ì˜ë¯¸)"
            confidence = "VERY LOW"

        # í…ìŠ¤íŠ¸ ë©”ì‹œì§€ ìƒì„±
        msg_lines = [ # ë©”ì‹œì§€ ë¼ì¸ë“¤
            "DATA QUALITY REPORT",
            f"Quality score : {total:.2f} / 1.00",
            f"Confidence    : {confidence}",
            f"{performance_msg}",
            "",
            "Details:",
            f"- Independence (1 - corr): {(1-avg_corr):.2f}",
            f"- Diversity (1 - PC1)   : {quality_score['diversity']:.2f}",
            f"- Sufficiency           : {quality_score['data_sufficiency']:.2f}",
            f"- Volatility dispersion : {quality_score['volatility_dispersion']:.2f}",
        ]
        if issues: # ë¬¸ì œì ì´ ìˆìœ¼ë©´
            msg_lines += ["", "Issues:"] + [f"{it}" for it in issues]
        if recommendations: # ê¶Œê³ ì‚¬í•­ì´ ìˆìœ¼ë©´
            msg_lines += ["", "Recommendations:"] + [f"{it}" for it in recommendations]
        quality_score["message"] = "\n".join(msg_lines) # ë©”ì‹œì§€ ê²°í•©
        quality_score["issues"] = issues # ë¬¸ì œì  ì €ì¥
        quality_score["recommendations"] = recommendations # ê¶Œê³ ì‚¬í•­ ì €ì¥
        quality_score["confidence"] = confidence # ì‹ ë¢°ë„ ì €ì¥

        # í•˜ë‹¨ ë Œë”ìš© ìš”ì•½ ë©”ì‹œì§€ ì¶”ê°€
        summary_line = ( # ìš”ì•½ ë¼ì¸
            f"DATA QUALITY total={quality_score['total']:.2f} "
            f"(corr={quality_score['correlation']:.2f}, diversity={quality_score['diversity']:.2f}, "
            f"suff={quality_score['data_sufficiency']:.2f}, volDisp={quality_score['volatility_dispersion']:.2f})"
        )
        self._push_msg(summary_line) # ìš”ì•½ ë©”ì‹œì§€ ì¶”ê°€
        if issues: # ë¬¸ì œì ì´ ìˆìœ¼ë©´
            self._push_msg("Issues: " + " | ".join(issues[:2])) # ë¬¸ì œì  ë©”ì‹œì§€ (ìµœëŒ€ 2ê°œ)
        if recommendations: # ê¶Œê³ ì‚¬í•­ì´ ìˆìœ¼ë©´
            self._push_msg("Recommendations: " + " | ".join(recommendations[:2])) # ê¶Œê³ ì‚¬í•­ ë©”ì‹œì§€ (ìµœëŒ€ 2ê°œ)

        return quality_score

    def adaptive_model_selection(self) -> Dict[str, Any]:
        # ë°ì´í„° íŠ¹ì„±ì— ë”°ë¥¸ ëª¨ë¸ ìë™ ì„ íƒ
        q = self.auto_detect_data_quality() # í’ˆì§ˆ ë¶„ì„
        if q["total"] < 0.3: # í’ˆì§ˆì´ ë‚®ìœ¼ë©´
            cfg = { # ì„¤ì •
                "model": "RandomForest",
                "reason": "í’ˆì§ˆ ë‚®ìŒ: ë‹¨ìˆœ ëª¨ë¸ì´ ìœ ë¦¬",
                "params": {"n_estimators": 100, "max_depth": 5, "note": "ê³¼ì í•© ë°©ì§€ìš© ê²½ëŸ‰ RF"},
            }
        elif q["correlation"] > 0.7: # ìƒê´€ê´€ê³„ê°€ ë†’ìœ¼ë©´
            cfg = { # ì„¤ì •
                "model": "FactorModel",
                "reason": "ìƒê´€ ë†’ìŒ: ê³µí†µ íŒ©í„° ì¶”ì¶œ ìœ ë¦¬",
                "params": {"n_factors": 3, "use_pca": True, "note": "PCA ê¸°ë°˜ íŒ©í„°"},
            }
        elif q["data_sufficiency"] < 0.5: # ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´
            cfg = { # ì„¤ì •
                "model": "SimpleLSTM",
                "reason": "í‘œë³¸ ë¶€ì¡±: ê²½ëŸ‰ ìˆœí™˜ëª¨ë¸",
                "params": {"hidden_size": 32, "num_layers": 1, "note": "ì—°ì‚° ê°€ë²¼ì›€"},
            }
        else: # ê·¸ ì™¸
            cfg = { # ì„¤ì •
                "model": "GRU",
                "reason": "í’ˆì§ˆ ì–‘í˜¸: GRU ì í•©",
                "params": {"hidden_size": 64, "num_layers": 2, "note": "ì„±ëŠ¥/ì•ˆì • ê· í˜•"},
            }

        lines = [ # ë©”ì‹œì§€ ë¼ì¸ë“¤
            "MODEL AUTO-SELECTION",
            f"Selected : {cfg['model']}",
            f"Reason   : {cfg['reason']}",
            "Params   : " + ", ".join([f"{k}={v}" for k, v in cfg["params"].items() if k != "note"]),
            f"Note     : {cfg['params'].get('note','')}",
            "",
            "Expected:",
            f"- Accuracy tier : {q['confidence']}",
            f"- Overfit risk  : {'Low' if q['data_sufficiency'] > 0.7 else 'Medium' if q['data_sufficiency'] > 0.4 else 'High'}",
        ]
        cfg["full_message"] = "\n".join(lines) # ì „ì²´ ë©”ì‹œì§€

        # í•˜ë‹¨ ë Œë”ìš©
        self._push_msg(f"MODEL SELECTED: {cfg['model']} - {cfg['reason']}") # ëª¨ë¸ ì„ íƒ ë©”ì‹œì§€
        return cfg

    def run_smart_analysis(self, **kwargs) -> Dict[str, Any]:
        # í’ˆì§ˆ ì²´í¬ í›„ ìë™ ëª¨ë¸ ì„ íƒ -> ë¶„ì„ ì‹¤í–‰
        q = self.auto_detect_data_quality() # í’ˆì§ˆ ë¶„ì„
        print(q["message"]) # í’ˆì§ˆ ë©”ì‹œì§€ ì¶œë ¥
        self._push_msg("Smart analysis started") # ìŠ¤ë§ˆíŠ¸ ë¶„ì„ ì‹œì‘ ë©”ì‹œì§€

        if q["total"] < 0.25: # í’ˆì§ˆì´ ë„ˆë¬´ ë‚®ìœ¼ë©´
            stop_msg = ( # ì¤‘ë‹¨ ë©”ì‹œì§€
                "ANALYSIS STOPPED: data quality too low.\n"
                "- Extend history (>= 2 years)\n"
                "- Increase universe size (>= 10)\n"
                "- Diversify sectors\n"
                f"Current quality: {q['total']:.2f} / 1.00 (min 0.25)"
            )
            self._push_msg("Stopped: quality below threshold") # ì¤‘ë‹¨ ë©”ì‹œì§€
            return { # ì‹¤íŒ¨ ê²°ê³¼ ë°˜í™˜
                "success": False,
                "message": stop_msg,
                "quality_score": q,
                "ui_messages": list(self.ui_messages),
            }

        cfg = self.adaptive_model_selection() # ëª¨ë¸ ì„ íƒ
        print(cfg["full_message"]) # ëª¨ë¸ ì„ íƒ ë©”ì‹œì§€ ì¶œë ¥

        print("Starting analysis...") # ë¶„ì„ ì‹œì‘ ì¶œë ¥
        self._push_msg("Running analysis...") # ë¶„ì„ ì‹¤í–‰ ë©”ì‹œì§€

        if cfg["model"] == "RandomForest": # RF ëª¨ë¸ì´ë©´
            result = self._predict_with_rf(horizon=kwargs.get("horizon", 5)) # RFë¡œ ì˜ˆì¸¡
            result_dict = { # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
                "prediction": {
                    "horizon": result.horizon,
                    "ic": result.ic,
                    "hit_rate": result.hit_rate,
                    "r2": result.r2,
                    "ic_by_date": {pd.Timestamp(k).strftime("%Y-%m-%d"): float(v) for k, v in result.ic_by_date.items()},
                }
            }
        else: # ë‹¤ë¥¸ ëª¨ë¸ì´ë©´
            result_dict = self.run_all(**kwargs) # ì „ì²´ ë¶„ì„ ì‹¤í–‰

        result_dict["quality_analysis"] = { # í’ˆì§ˆ ë¶„ì„ ê²°ê³¼ ì¶”ê°€
            "score": q["total"],
            "confidence": q["confidence"],
            "issues": q["issues"],
            "recommendations": q["recommendations"],
        }
        result_dict["model_used"] = cfg # ì‚¬ìš©ëœ ëª¨ë¸ ì •ë³´

        ic_val = result_dict.get("prediction", {}).get("ic", 0.0) or 0.0 # IC ê°’
        hit_val = result_dict.get("prediction", {}).get("hit_rate", 0.0) or 0.0 # ì ì¤‘ë¥  ê°’
        self._push_msg(f"RESULT: IC={ic_val:.3f}, Hit={hit_val:.3f}, Horizon={kwargs.get('horizon', 5)}") # ê²°ê³¼ ë©”ì‹œì§€

        if ic_val > 0.05: # ICê°€ 0.05 ì´ˆê³¼ë©´
            print( # ê²°ê³¼ ì¶œë ¥
                "ANALYSIS RESULT\n"
                f"IC (test): {ic_val:.3f}\n"
                "Decision : investable range" if ic_val > 0.05 else "Decision : borderline"
            )
        else: # ê·¸ ì™¸
            print( # ì €ì„±ëŠ¥ ê²°ê³¼ ì¶œë ¥
                "ANALYSIS RESULT (LOW PERFORMANCE)\n"
                f"IC (test): {ic_val:.3f}\n"
                "Potential causes: " + (", ".join(q["issues"][:2]) if q["issues"] else "data characteristics") + "\n"
                "Next steps:\n" + ("\n".join(q["recommendations"][:3]) if q["recommendations"] else "augment data")
            )

        # í•˜ë‹¨ ë©”ì‹œì§€ í¬í•¨
        result_dict["ui_messages"] = list(self.ui_messages) # UI ë©”ì‹œì§€ ì¶”ê°€
        return result_dict

    # í’ˆì§ˆ ë¶„ì„ê³¼ ì•ˆì •ì„± ë¶„ì„ì„ í¬í•¨í•œ ì „ì²´ ML ë¶„ì„
    def run_all_with_analysis(self, **kwargs) -> Dict[str, Any]:
        # í™•ì¥ëœ ë¶„ì„ - í’ˆì§ˆ, ì•ˆì •ì„± í¬í•¨
        # ê¸°ë³¸ run_all ì‹¤í–‰
        result = self.run_all(**kwargs) # ê¸°ë³¸ ë¶„ì„ ì‹¤í–‰
        
        # í’ˆì§ˆ ë¶„ì„ ì¶”ê°€
        quality = self.auto_detect_data_quality() # í’ˆì§ˆ ë¶„ì„
        
        # Walk-forward ë¶„ì„ ì¶”ê°€
        walk_forward = walk_forward_analysis( # ì›Œí¬í¬ì›Œë“œ ë¶„ì„
            self,
            horizon=kwargs.get('horizon', 5),
            use_gru=kwargs.get('use_gru', True),
            quick=True
        )
        
        # ê²°ê³¼ì— ë¶„ì„ ì •ë³´ ì¶”ê°€
        result['quality_analysis'] = { # í’ˆì§ˆ ë¶„ì„ ì¶”ê°€
            'score': quality['total'],
            'confidence': quality['confidence'],
            'issues': quality['issues'],
            'recommendations': quality['recommendations'],
            'details': {
                'correlation': quality['correlation'],
                'diversity': quality['diversity'],
                'data_sufficiency': quality['data_sufficiency'],
                'volatility_dispersion': quality['volatility_dispersion']
            }
        }
        
        result['stability_analysis'] = walk_forward # ì•ˆì •ì„± ë¶„ì„ ì¶”ê°€
        
        # ì½˜ì†” ì¶œë ¥ìš© ìš”ì•½ ì •ë³´ ì¶”ê°€
        r2_val = result.get('prediction', {}).get('r2', None) # R2 ê°’
        r2_txt = f"{r2_val:.3f}" if (r2_val is not None) else "N/A" # R2 í…ìŠ¤íŠ¸
        result['analysis_summary'] = f"""
ë°ì´í„° í’ˆì§ˆ ë¶„ì„ ê²°ê³¼                              
í’ˆì§ˆ ì ìˆ˜: {quality['total']:.2f}/1.00
ì‹ ë¢°ë„: {quality['confidence']}
ì˜ˆìƒ IC: {quality.get('performance_msg', 'N/A')}

ì„¸ë¶€ ì ìˆ˜:
- ì¢…ëª© ë…ë¦½ì„±: {(1-quality['correlation']):.2f}/1.00
- ì¢…ëª© ë‹¤ì–‘ì„±: {quality['diversity']:.2f}/1.00
- ë°ì´í„° ì¶©ë¶„: {quality['data_sufficiency']:.2f}/1.00
- ë³€ë™ì„± ë¶„ì‚°: {quality['volatility_dispersion']:.2f}/1.00

ì•ˆì •ì„± ë¶„ì„ ê²°ê³¼                                

{walk_forward.get('summary', 'Walk-forward ë¶„ì„ ì‹¤íŒ¨')}

ì˜ˆì¸¡ ì„±ëŠ¥ ê²°ê³¼                                  

ì‹¤ì œ IC: {result.get('prediction', {}).get('ic', 0):.3f}
ì ì¤‘ë¥ : {result.get('prediction', {}).get('hit_rate', 0)*100:.1f}%
RÂ²: {r2_txt}

íˆ¬ì ê°€ëŠ¥ ì—¬ë¶€: {walk_forward.get('investment_grade', 'UNKNOWN')}
{walk_forward.get('message', '')}
        """ # ë¶„ì„ ìš”ì•½

        # ì½˜ì†”ì— ì¶œë ¥
        print(result['analysis_summary']) # ìš”ì•½ ì¶œë ¥
        
        # í•˜ë‹¨ ë©”ì‹œì§€ ì—…ë°ì´íŠ¸
        summary_lines = [ # ìš”ì•½ ë¼ì¸ë“¤
            f"í’ˆì§ˆ: {quality['total']:.2f} ({quality['confidence']})",
            f"ICìƒ¤í”„: {walk_forward['metrics']['ic_sharpe']:.2f}" if walk_forward.get('success') else "ì•ˆì •ì„± ë¶„ì„ ì‹¤íŒ¨",
            f"íˆ¬ìë“±ê¸‰: {walk_forward.get('investment_grade', 'N/A')}"
        ]
        self._push_msg(" | ".join(summary_lines)) # ìš”ì•½ ë©”ì‹œì§€ ì¶”ê°€
        
        return result
    
    # ml_charts.py í˜¸í™˜ìš© alias
    def run_ml_analysis(self, **kwargs) -> Dict[str, Any]:
        # run_all_with_analysisì˜ alias - í™•ì¥ ë¶„ì„ í¬í•¨
        return self.run_all_with_analysis(**kwargs)

# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ 

def walk_forward_analysis(
    analyzer: MLAnalyzer, # ë¶„ì„ê¸° ê°ì²´
    *,
    n_windows: int = 10, # ìœˆë„ìš° ê°œìˆ˜
    test_ratio: float = 0.2, # í…ŒìŠ¤íŠ¸ ë¹„ìœ¨
    horizon: int = 5, # ì˜ˆì¸¡ ê¸°ê°„
    use_gru: bool = True, # GRU ì‚¬ìš© ì—¬ë¶€
    quick: bool = True, # ë¹ ë¥¸ ëª¨ë“œ
    min_train_days: int = 100, # ìµœì†Œ í›ˆë ¨ ì¼ìˆ˜
    **kwargs
) -> Dict[str, Any]:
    
    returns = analyzer.returns # ìˆ˜ìµë¥  ë°ì´í„°
    n_days = len(returns) # ì „ì²´ ì¼ìˆ˜
    n_stocks = returns.shape[1] # ì¢…ëª© ìˆ˜
    
    if n_days < min_train_days + 20: # ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´
        return { # ì‹¤íŒ¨ ê²°ê³¼ ë°˜í™˜
            'success': False,
            'message': f'ë°ì´í„° ë¶€ì¡±: {n_days}ì¼ (ìµœì†Œ {min_train_days + 20}ì¼ í•„ìš”)',
            'metrics': {
                'ic_mean': 0.0,
                'ic_std': 1.0,
                'ic_sharpe': 0.0
            }
        }
    
    total_test_size = int(n_days * 0.4) # ì „ì²´ í…ŒìŠ¤íŠ¸ í¬ê¸°
    window_size = max(1, total_test_size // max(1, n_windows)) # ìœˆë„ìš° í¬ê¸°
    
    ic_results = [] # IC ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    hit_results = [] # ì ì¤‘ë¥  ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    window_details = [] # ìœˆë„ìš° ì„¸ë¶€ì‚¬í•­
    
    for i in range(n_windows): # ê° ìœˆë„ìš°ì— ëŒ€í•´
        try:
            test_end = n_days - (i * window_size) # í…ŒìŠ¤íŠ¸ ëì 
            test_start = test_end - window_size # í…ŒìŠ¤íŠ¸ ì‹œì‘ì 
            train_end = test_start # í›ˆë ¨ ëì 
            train_start = max(0, train_end - min_train_days) # í›ˆë ¨ ì‹œì‘ì 
            
            if train_end - train_start < min_train_days: # í›ˆë ¨ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´
                continue # ê±´ë„ˆë›°ê¸°
                
            train_returns = returns.iloc[train_start:train_end] # í›ˆë ¨ ìˆ˜ìµë¥ 
            temp_analyzer = MLAnalyzer(train_returns) # ì„ì‹œ ë¶„ì„ê¸°
            
            tuned = temp_analyzer._auto_tune( # ìë™ íŠœë‹
                horizon=horizon,
                seq_len=None, epochs=None, hidden_size=None, dropout=None,
                hrp_max_weight=None, hrp_blend_to_equal=None,
                long_tau=None, short_tau=None
            )
            
            if quick: # ë¹ ë¥¸ ëª¨ë“œë©´
                tuned['epochs'] = 10 # ì—í­ 10ìœ¼ë¡œ ì„¤ì •
            
            pred_result = temp_analyzer.predict_next( # ì˜ˆì¸¡ ì‹¤í–‰
                horizon=horizon,
                use_gru=use_gru,
                seq_len=tuned['seq_len'],
                epochs=tuned['epochs'],
                hidden_size=tuned['hidden_size'],
                dropout=tuned['dropout'],
                **kwargs
            )
            
            test_returns = returns.iloc[test_start:test_end] # í…ŒìŠ¤íŠ¸ ìˆ˜ìµë¥ 
            if len(test_returns) > horizon: # í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ë©´
                ic = pred_result.ic if not np.isnan(pred_result.ic) else 0.0 # IC ê°’
                hit = pred_result.hit_rate if not np.isnan(pred_result.hit_rate) else 0.5 # ì ì¤‘ë¥ 
                
                ic_results.append(ic) # IC ê²°ê³¼ ì¶”ê°€
                hit_results.append(hit) # ì ì¤‘ë¥  ê²°ê³¼ ì¶”ê°€
                
                window_details.append({ # ìœˆë„ìš° ì„¸ë¶€ì‚¬í•­ ì¶”ê°€
                    'window': i + 1,
                    'train_period': f'{train_start}-{train_end}',
                    'test_period': f'{test_start}-{test_end}',
                    'ic': ic,
                    'hit_rate': hit
                })
                
        except Exception as e:
            logger.debug(f"Window {i} failed: {e}") # ìœˆë„ìš° ì‹¤íŒ¨ ë¡œê·¸
            continue
    
    if not ic_results: # IC ê²°ê³¼ê°€ ì—†ìœ¼ë©´
        return { # ì‹¤íŒ¨ ê²°ê³¼ ë°˜í™˜
            'success': False,
            'message': 'ì›Œí¬í¬ì›Œë“œ ë¶„ì„ ì‹¤íŒ¨',
            'metrics': {
                'ic_mean': 0.0,
                'ic_std': 1.0,
                'ic_sharpe': 0.0
            }
        }
    
    ic_array = np.array(ic_results) # IC ë°°ì—´
    hit_array = np.array(hit_results) # ì ì¤‘ë¥  ë°°ì—´
    
    ic_mean = float(np.mean(ic_array)) # IC í‰ê· 
    ic_std = float(np.std(ic_array)) if float(np.std(ic_array)) > 0 else 1e-9 # IC í‘œì¤€í¸ì°¨
    ic_sharpe = ic_mean / ic_std if ic_std > 0 else 0.0 # IC ìƒ¤í”„ ë¹„ìœ¨
    positive_ic_ratio = float(np.mean(ic_array > 0)) # ì–‘ìˆ˜ IC ë¹„ìœ¨
    
    cv = ic_std / abs(ic_mean) if abs(ic_mean) > 0.01 else float('inf') # ë³€ë™ê³„ìˆ˜
    
    if ic_sharpe >= 2.0 and positive_ic_ratio >= 0.7: # ë§¤ìš° ì¢‹ì€ ì„±ëŠ¥
        investment_grade = "EXCELLENT"
        message = "ì‹¤ì „ íˆ¬ì ê°€ëŠ¥ - ì•ˆì •ì ì¸ ìˆ˜ìµ ì˜ˆìƒ"
    elif ic_sharpe >= 1.0 and positive_ic_ratio >= 0.6: # ì¢‹ì€ ì„±ëŠ¥
        investment_grade = "GOOD"
        message = "íˆ¬ì ê°€ëŠ¥ - ì ì ˆí•œ ë¦¬ìŠ¤í¬ ê´€ë¦¬ í•„ìš”"
    elif ic_sharpe >= 0.5 and positive_ic_ratio >= 0.5: # ë³´í†µ ì„±ëŠ¥
        investment_grade = "FAIR"
        message = "ì¡°ê±´ë¶€ íˆ¬ì ê°€ëŠ¥ - ì¶”ê°€ ê²€ì¦ í•„ìš”"
    else: # ë‚˜ìœ ì„±ëŠ¥
        investment_grade = "POOR"
        message = "ì‹¤ì „ ì‚¬ìš© ë¶ˆê°€ - ëª¨ë¸ ì¬ê²€í†  í•„ìš”"
    
    result = { # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        'success': True,
        'investment_grade': investment_grade,
        'message': message,
        'metrics': { # ì§€í‘œë“¤
            'ic_mean': ic_mean,
            'ic_std': ic_std,
            'ic_sharpe': ic_sharpe,
            'positive_ic_ratio': positive_ic_ratio,
            'hit_rate_mean': float(np.mean(hit_array)) if len(hit_array) else float('nan'),
            'market_regime_stability': 1.0 / (1.0 + cv),
            'n_windows_tested': len(ic_results)
        },
        'window_details': window_details,
        'summary': f"""
ì•ˆì •ì„± ë¶„ì„ ê²°ê³¼:
- í‰ê·  IC: {ic_mean:.3f} {'(ì–‘í˜¸)' if ic_mean > 0.05 else '(ë¯¸í¡)'}
- IC í‘œì¤€í¸ì°¨: {ic_std:.3f} {'(ì•ˆì •ì )' if ic_std < 0.1 else '(ë¶ˆì•ˆì •)'}
- IC ìƒ¤í”„: {ic_sharpe:.2f} {'(ë§¤ìš° ì¢‹ìŒ)' if ic_sharpe > 2 else '(ì¢‹ìŒ)' if ic_sharpe > 1 else '(ë³´í†µ)' if ic_sharpe > 0.5 else '(ë‚˜ì¨)'}
- ì–‘ìˆ˜ IC ë¹„ìœ¨: {positive_ic_ratio*100:.0f}% {'(ëŒ€ë¶€ë¶„ ê¸°ê°„ì—ì„œ ìˆ˜ìµ)' if positive_ic_ratio > 0.7 else '(ë³€ë™ì )' if positive_ic_ratio > 0.5 else '(ë¶ˆì•ˆì •)'}
- ì‹œì¥ êµ­ë©´ë³„ í¸ì°¨: {cv:.2f} {'(êµ­ë©´ ë¬´ê´€í•˜ê²Œ ì‘ë™)' if cv < 0.5 else '(êµ­ë©´ ì˜ì¡´ì )'}
â†’ {message}
        """ # ìš”ì•½ ì •ë³´
    }
    
    return result

# ê°„í¸ í’ˆì§ˆ ì²´í¬
def quick_quality_check(returns: pd.DataFrame) -> float:
    # í’ˆì§ˆ ì ìˆ˜ë§Œ ë¹ ë¥´ê²Œ ì¶œë ¥
    analyzer = AdaptiveMLAnalyzer(returns) # ì ì‘í˜• ë¶„ì„ê¸° ìƒì„±
    quality = analyzer.auto_detect_data_quality() # í’ˆì§ˆ ë¶„ì„
    print(quality["message"]) # í’ˆì§ˆ ë©”ì‹œì§€ ì¶œë ¥
    return quality["total"] # ì´ì  ë°˜í™˜

# ë°ì´í„°ì…‹ í’ˆì§ˆ ë¹„êµ
def compare_multiple_datasets(datasets: Dict[str, pd.DataFrame], quiet: bool = False) -> pd.DataFrame:
    # ì—¬ëŸ¬ ì„¹í„°/ë°”ìŠ¤ì¼“ returnsë¥¼ ë°›ì•„ í’ˆì§ˆ ì ìˆ˜ë¥¼ ë¹„êµ
    results = [] # ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    for name, rets in datasets.items(): # ê° ë°ì´í„°ì…‹ì— ëŒ€í•´
        analyzer = AdaptiveMLAnalyzer(rets) # ì ì‘í˜• ë¶„ì„ê¸° ìƒì„±
        q = analyzer.auto_detect_data_quality() # í’ˆì§ˆ ë¶„ì„
        results.append({ # ê²°ê³¼ ì¶”ê°€
            "ì„¹í„°": name,
            "ì¢…ëª©ìˆ˜": rets.shape[1],
            "ê¸°ê°„(ì¼)": len(rets),
            "í’ˆì§ˆì ìˆ˜": float(q["total"]),
            "ì‹ ë¢°ë„": q["confidence"],
            "ì˜ˆìƒIC": ("0.10+" if q["total"] > 0.7 else "0.05~0.10" if q["total"] > 0.5 else "<0.05"),
            "íˆ¬ìê°€ëŠ¥": ("YES" if q["total"] > 0.5 else "Maybe" if q["total"] > 0.3 else "NO"),
        })
    df = pd.DataFrame(results).sort_values("í’ˆì§ˆì ìˆ˜", ascending=False).reset_index(drop=True) # DataFrame ìƒì„±
    if not quiet: # ì¡°ìš©í•œ ëª¨ë“œê°€ ì•„ë‹ˆë©´
        print("DATASET QUALITY COMPARISON") # ë¹„êµ ì œëª© ì¶œë ¥
        print(df.to_string(index=False)) # DataFrame ì¶œë ¥
    return df